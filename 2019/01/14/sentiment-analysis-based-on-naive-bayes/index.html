<!DOCTYPE html><html lang="en,zh-cn,default"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Sentiment Analysis Based on Naive Bayes · Infinite.ft的博客</title><meta name="description" content="Sentiment Analysis Based on Naive Bayes - infinite.ft"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://free-free.github.io/atom.xml" title="Infinite.ft的博客"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">HOME</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/categories/math" target="_self" class="nav-list-link">MATH</a></li><li class="nav-list-item"><a href="/categories/tech" target="_self" class="nav-list-link">TECH</a></li><li class="nav-list-item"><a href="/categories/mylife" target="_self" class="nav-list-link">NOTES</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Sentiment Analysis Based on Naive Bayes</h1><div class="post-info">Jan 14, 2019</div><div class="post-content"><h3 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h3><p>文本分类一直是自然处理领域内重要的方向之一，其主要目标是在给定类别的情况下，预测一段文本相应的类别标签。文本分类广泛应用于垃圾邮件检测、文本话题分类、语言类别识别等领域内。而文本情感分析(Sentiment Analysis,SA)则是文本分类里另一个重要的课题，它是对带有主观感情色彩的文本进行分析、处理、归纳和推理，最后得出文本中的所蕴含的主观情绪。情感分析分析的一个基本步骤便是对文本中的某段已知文字的两极性进行分类(i.e., 积极或消极)。更高级的“超出两极性”的情感分析还会寻找更复杂的情感状态，例如”生气“、”悲伤“、“快乐”等等[1]。文本情感分析有很多方法，例如隐含语意分析(Latent Semantic Analysis, LSA)、SVM(Support Vector Machines)、Naive Bayes等。Naive Bayes常作为文本分类和情感分析常用的baseline，本文将介绍如何使用Naive Bayes文本情绪极性分类。</p>
<h3 id="2-Naive-Bayes-情感极性分类"><a href="#2-Naive-Bayes-情感极性分类" class="headerlink" title="2. Naive Bayes 情感极性分类"></a>2. Naive Bayes 情感极性分类</h3><h4 id="2-1-数据准备"><a href="#2-1-数据准备" class="headerlink" title="2.1 数据准备"></a>2.1 数据准备</h4><p>数据采用中文对话情绪语料，该数据集来源于文章[2]。该数据由<a href="https://raw.githubusercontent.com/xxxspy/Chinese_conversation_sentiment/master/sentiment_XS_test.txt" target="_blank" rel="external">sentiment_XS_test.txt</a>和<a href="https://raw.githubusercontent.com/xxxspy/Chinese_conversation_sentiment/master/sentiment_XS_30k.txt" target="_blank" rel="external">sentiment_XS_30k.txt</a>两个文件组成。<code>sentiment_XS_test.txt</code> 包含11577个手动标记的实例，<code>sentiment_XS_30k.txt</code>包含30k个自动标记的实例。每个文件均包含两列<code>labels</code>和<code>text</code>，<code>text</code>每一行包含已经切好的词语，词语之间用空格隔开，<code>labels</code>对应<code>text</code>所对应的情感极性，有<code>positive</code>和<code>negative</code>两个选择。所有数据均来自于人机对话日志，并用<code>JieBa</code>工具进行分词。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>labels</th>
<th>text</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>positive</td>
<td>奖励 就是 亲</td>
</tr>
<tr>
<td>1</td>
<td>positive</td>
<td>谢谢 妹妹 加 朋友</td>
</tr>
<tr>
<td>2</td>
<td>positive</td>
<td>喜欢 吃 南浮</td>
</tr>
<tr>
<td>3</td>
<td>positive</td>
<td>好 吧 小 很 强悍 怕 不</td>
</tr>
<tr>
<td>4</td>
<td>negative</td>
<td>他 女朋友 旁边 所以 不 方便 跟 说话</td>
</tr>
<tr>
<td>5</td>
<td>positive</td>
<td>讲 的话 真 幽默</td>
</tr>
<tr>
<td>6</td>
<td>negative</td>
<td>真的 那 好 心疼</td>
</tr>
<tr>
<td>7</td>
<td>positive</td>
<td>无所不能 可是 刚才 差点 要 上 百度</td>
</tr>
<tr>
<td>8</td>
<td>positive</td>
<td>就是 漂亮 嘛 完美</td>
</tr>
<tr>
<td>9</td>
<td>negative</td>
<td>跟 一个 低级 计算机系统 说话 要 那么</td>
</tr>
</tbody>
</table>
</div>
<h4 id="2-2-所需工具"><a href="#2-2-所需工具" class="headerlink" title="2.2 所需工具"></a>2.2 所需工具</h4><ul>
<li>pandas: 导入数据 </li>
<li>sklearn: 处理数据，进行模型学习</li>
<li>numpy: 配合pandas使用</li>
</ul>
<h4 id="2-3-Naive-Bayes情感极性分类器"><a href="#2-3-Naive-Bayes情感极性分类器" class="headerlink" title="2.3  Naive Bayes情感极性分类器"></a>2.3  Naive Bayes情感极性分类器</h4><p>导入相关的库</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</div><div class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</div></pre></td></tr></table></figure>
<p>导入数据并进行预处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">PATH = <span class="string">'../input'</span></div><div class="line"><span class="comment"># 导入训练数据和验证数据</span></div><div class="line">trn = pd.read_csv(<span class="string">f'<span class="subst">&#123;PATH&#125;</span>/sentiment_XS_30k.txt'</span>)</div><div class="line">val = pd.read_csv(<span class="string">f'<span class="subst">&#123;PATH&#125;</span>/sentiment_XS_test.txt'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 得到输入文本和对应的情感极性，并情感极性转换为1（positive),0(negative)</span></div><div class="line">trn_x = trn[<span class="string">'text'</span>].values</div><div class="line">trn_y = trn[<span class="string">'labels'</span>].map(&#123;<span class="string">'positive'</span>:<span class="number">1</span>, <span class="string">'negative'</span>:<span class="number">0</span>&#125;).values</div><div class="line">val_x = trn[<span class="string">'text'</span>].values</div><div class="line">val_y = trn[<span class="string">'labels'</span>].map(&#123;<span class="string">'positive'</span>:<span class="number">1</span>, <span class="string">'negative'</span>:<span class="number">0</span>&#125;).values</div><div class="line"></div><div class="line"><span class="comment"># 抽取文本特征，将文本转换为词频向量。词频向量的维度大</span></div><div class="line"><span class="comment"># 小等于训练样本中unique token的个数相同。</span></div><div class="line"><span class="comment"># 由于数据集已经事先切分好，并用空格隔开，</span></div><div class="line"><span class="comment"># 所以tokenizer只需要进行简单空格切分即可</span></div><div class="line">vectorizer = CountVectorizer(tokenizer=<span class="keyword">lambda</span> text: text.split(<span class="string">' '</span>))</div><div class="line">trn_term_doc = vectorizer.fit_transform(trn_x)</div><div class="line">val_term_doc = vectorizer.transform(val_x)</div></pre></td></tr></table></figure>
<p>计算Naive Bayes参数：</p>
<p>为了在预测阶段便于计算最终的类别，在这里定义一个概念叫token的<strong>对数计数比(log-count ratio) r</strong>和类别\(Y\)的<strong>对数计数比 b</strong>，具体请参考[3]。其定义如下：</p>
<script type="math/tex; mode=display">
r_j = \log{\frac{p(X^{(j)}=x^{(j)} | Y=positive)}{p(X^{(j)}=x^{(j)} | Y=negative)}},j = 1, 2, \dots, n \\
r = [r_1,r_2, \dots, r_n] \\ 
b = \log{\frac{\sum_i^N{I(y_i = positive)}}{\sum_i^N{I(y_i = negative)}}}</script><p>其中\(n\)为特征个数，在这里即为词频向量的大小，\(N\)为训练数据大小。这样定义的好处在预测阶段会显现出来。<strong>在这里我们假设每一段文本中每个词最多只出现一次，即词频向量为0,1组成的向量</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 计算得到词频向量大小</span></div><div class="line">dim_sz = trn_term_doc[<span class="number">0</span>].shape[<span class="number">1</span>]</div><div class="line"><span class="comment"># 计算条件概率P(X_j = x_j| P=c_k)。需要注意的时，</span></div><div class="line"><span class="comment"># 计算条件概率的时候使用了Laplace平滑</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_con_proba</span><span class="params">(x, y, y_class)</span>:</span></div><div class="line">    p = x[y == y_class].sum(<span class="number">0</span>)</div><div class="line">    <span class="keyword">return</span> (p + <span class="number">1</span>) / ((y == y_class).sum() + dim_sz)</div><div class="line"><span class="comment"># 计算P(Y=c_k),也是使用了Laplace平滑</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_y_proba</span><span class="params">(y, y_class)</span>:</span></div><div class="line">    <span class="keyword">return</span> ((y == y_class).sum() + <span class="number">1</span>) / (len(y) + <span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="comment"># 计算r和b</span></div><div class="line">r = np.log(calc_con_proba(trn_term_doc, trn_y, <span class="number">1</span>) / </div><div class="line">  calc_con_proba(trn_term_doc, trn_y, <span class="number">0</span>))</div><div class="line">b = np.log(calc_y_proba(trn_y, <span class="number">1</span>)  / calc_y_proba(trn_y, <span class="number">0</span>))</div><div class="line"></div><div class="line"><span class="comment"># 通过求出r和b，其实已经计算出Naive Bayes的参数，</span></div><div class="line"><span class="comment"># 既得到一个Naive Bayes分类器</span></div></pre></td></tr></table></figure>
<p>使用Naive Bayes对测试数据进行预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 得出预测结果</span></div><div class="line">pre_preds = val_term_doc @ r.T + b</div><div class="line">preds = pre_preds.T &gt; <span class="number">0</span></div><div class="line"><span class="comment"># 计算accuracy</span></div><div class="line">acc = (preds == val_y).mean()</div><div class="line"><span class="comment"># 79.69%</span></div><div class="line"></div><div class="line"><span class="comment"># 计算输出全为1，或者全为0时对应的accuracy，作为对比</span></div><div class="line">acc2 = (val_y == <span class="number">1</span>).sum() / len(val_y)</div><div class="line"><span class="comment"># 45.82%</span></div><div class="line">acc3 = (val_y == <span class="number">0</span>).sum() / len(val_y)</div><div class="line"><span class="comment"># 54.18%</span></div></pre></td></tr></table></figure>
<p>进行预测时，只要将每个输入文本对应的词频向量与\(r\)內积，并加上\(b\)，最终结果大于0说明为<code>positive</code>，否则为<code>negative</code>。即</p>
<script type="math/tex; mode=display">
p = \log{(P(Y=1) \prod_{j=1}^{n}{P(X^{(j)}=x^{(j)} | Y=positive)})}\\ - \log {(P(Y=0) \prod_{j=1}^{n}{P(X^{(j)}=x^{(j)} | Y=negative)})}</script><p>如果\(p \gt 0\)说明正类的后验概率大于负类的后验概率，即最终输出为正类，否则输出负类。</p>
<p>可以看出上述的预测过程其实等价于\(y = sign(Wx + b)\)，只不过参数\(W,b\)是通过Bayes理论计算得到。那可不可以通过学习得到这些参数呢？答案是肯定的。这便是Logistic Regression。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">log = LogisticRegression(C=<span class="number">1e2</span>, dual=<span class="keyword">True</span>)</div><div class="line">log.fit(trn_term_doc, trn_y)</div><div class="line">preds = log.predict(val_term_doc)</div><div class="line">(preds == val_y).mean()</div><div class="line"><span class="comment"># 78.77%</span></div></pre></td></tr></table></figure>
<p>从上述结果可以看出，通过理论计算的参数与学习的参数在分类准确率上差别不大。</p>
<h3 id="参考-amp-荐读："><a href="#参考-amp-荐读：" class="headerlink" title="参考&amp;荐读："></a>参考&amp;荐读：</h3><p><a href="https://zh.wikipedia.org/wiki/%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90" target="_blank" rel="external">[1] 文本情感分析</a></p>
<p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7820437" target="_blank" rel="external">[2] Sentiment Classification with Convolutional Neural Networks: An Experimental Study on a Large-Scale Chinese Conversation Corpus</a><br><a href="https://www.aclweb.org/anthology/P12-2018" target="_blank" rel="external">[3] Baselines and Bigrams: Simple, Good Sentiment and Topic Classification</a></p>
</div></article></div></main><footer><div class="paginator"><a href="/2019/03/16/the-several-tricks-for-jupyter-notebook/" class="prev">PREV</a><a href="/2019/01/12/naive-bayes-model/" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2019 <a href="https://free-free.github.io">infinite.ft</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>