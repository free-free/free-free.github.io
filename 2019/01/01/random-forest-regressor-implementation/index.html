<!DOCTYPE html><html lang="en,zh-cn,default"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Random Forest Regressor Implementation · Infinite.ft的博客</title><meta name="description" content="Random Forest Regressor Implementation - infinite.ft"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://free-free.github.io/atom.xml" title="Infinite.ft的博客"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">HOME</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/categories/math" target="_self" class="nav-list-link">MATH</a></li><li class="nav-list-item"><a href="/categories/tech" target="_self" class="nav-list-link">TECH</a></li><li class="nav-list-item"><a href="/categories/mylife" target="_self" class="nav-list-link">NOTES</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Random Forest Regressor Implementation</h1><div class="post-info">Jan 1, 2019</div><div class="post-content"><h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a><strong>一、前言</strong></h2><p>回归随机森林作为一种机器学习和数据分析领域常用且有效的算法，对其原理和代码实现过程的掌握是非常有必要的。为此，本文将着重介绍从零开始实现回归随机森林的过程，对于随机森林和决策树的相关理论原理将不做太深入的描述。本文的目的只是为了演示回归随机森林主要功能的具体实现过程，在实现过程中不会考虑代码性能，会更加注重代码可读性。</p>
<p>实现语言：<em>Python</em></p>
<p>依赖：<em>pandas, numpy</em></p>
<h2 id="二、原理介绍"><a href="#二、原理介绍" class="headerlink" title="二、原理介绍"></a><strong>二、原理介绍</strong></h2><p>随机森林属于Bagging类算法，而Bagging 又属于<a href="http://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E9%259B%2586%25E6%2588%2590%25E5%25AD%25A6%25E4%25B9%25A0/3440721" target="_blank" rel="external">集成学习</a>一种方法（集成学习方法大致分为Boosting和Bagging方法，两个方法的不同请参考[10]），集成学习的大致思路是训练多个弱模型打包起来组成一个强模型，强模型的性能要比单个弱模型好很多（三个臭皮匠顶一个诸葛亮。注意：这里的弱和强是相对的），其中的弱模型可以是决策树、SVM等模型，在随机森林中，弱模型选用决策树。</p>
<p>在训练阶段，随机森林使用bootstrap采样从输入训练数据集中采集多个不同的子训练数据集来依次训练多个不同决策树；在预测阶段，随机森林将内部多个决策树的预测结果取平均得到最终的结果。本文主要介绍回归随机森林从零实现的过程，实现的RFR(回归随机森林)具有以下功能：</p>
<ul>
<li><strong>模型训练</strong></li>
<li><strong>模型数据预测</strong></li>
<li><strong>计算feature importance</strong></li>
</ul>
<p><strong>2.1 模型训练</strong></p>
<p><strong>2.1.1 基础理论</strong></p>
<p>本文实现的<strong>RFR</strong>是将多个<strong>二叉决策树（</strong>即<strong>CART，</strong>这也是<em>sklearn,spark</em>内部实现的模型<strong>）</strong>打包组合而成的，训练RFR便是训练多个二叉决策树。在训练二叉决策树模型的时候需要考虑<strong>怎样选择切分变量(特征）、切分点</strong>以及<strong>怎样衡量一个切分变量、切分点的好坏</strong>。针对于切分变量和切分点的选择，本实现采用穷举法，即遍历每个特征和每个特征的所有取值，最后从中找出最好的切分变量和切分点；针对于切分变量和切分点的好坏，一般以切分后节点的不纯度来衡量，即各个子节点不纯度的加权和\(G(x_i, v_{ij})\) ，其计算公式如下：</p>
<p><script type="math/tex">G(x_i, v_{ij}) = \frac{n_{left}}{N_s}H(X_{left}) +\frac{n_{right}}{N_s}H(X_{right})</script>                                               (2-1)</p>
<p>其中， \(x_i\) 为某一个切分变量， \(v_{ij}\) 为切分变量的一个切分值， \(n_{left}, n_{right},N_s\) 分别为切分后左子节点的训练样本个数、右子节点的训练样本个数以及当前节点所有训练样本个数， \(X_{left},X_{right}\)  分为左右子节点的训练样本集合， \(H(X)\) 为衡量节点不纯度的函数(impurity function/criterion)，分类和回归任务一般采用不同的不纯度函数，在分类和回归常采用的不纯度函数有以下四种：</p>
<p><img src="https://pic2.zhimg.com/80/v2-e5dec7d64aef5eca9ca54db57ffada75_hd.jpg" alt="img">图 1. 四种不同的impurity function</p>
<ul>
<li>备注1：</li>
</ul>
<p>Gini不纯度只适用于分类任务， \(X_m\) 为当前节点的训练样本集合， \(p_{mk}\) 为当前节点训练样本中目标变量取值\(p_{mk} = \frac{N_{mk}}{N_m}]\)出现的概率。</p>
<ul>
<li>备注2：</li>
</ul>
<p>信息熵只适用于分类任务， 其他同备注1。</p>
<ul>
<li>备注3</li>
</ul>
<p>MSE只适用于回归任务， \(\bar{y}_m\) 为当前节点样本目标变量的平均值。</p>
<ul>
<li>备注4</li>
</ul>
<p>MAE只适用于回归任务，\(\bar{y}_m\) 为当前节点样本目标变量的平均值。</p>
<ul>
<li>备注5</li>
</ul>
<p>在<em>sklearn</em>内部，<em>DecisionTreeClassifier, RandomForestClassifier</em>等基于决策树的分类模型默认使用<strong>*‘gini’*</strong>作为impurity function<em>，</em>也可通过<em>criterion</em>参数指定为<em>‘*<em>entropy’ *</em>；</em>而<em>DecisionTreeRegressor, RandomForestRegressor</em>等基于决策树的回归模型默认使用<strong>*‘mse’*</strong>作为impurity function<em>，</em>也可通过<em>criterion</em>参数指定为<strong>*‘mae’</strong>。*</p>
<p>决策树中某一节点的训练过程在数学等价于下面优化问题：</p>
<p><script type="math/tex">(x^*, v^*) = \operatorname{argmin}_{x, v}  G(x_i, v_{ij})</script>                                               (2-2)</p>
<p>即寻找\(G\) 最小的切分变量和切分点。</p>
<p><strong>在本文实现的回归随机森林中，\(H(X)\) 选用MSE，即针对某一切分点</strong></p>
<p><script type="math/tex">G(x, v) = \frac{1}{N_s} (\sum_{y_i \in X_{left}} (y_i - \bar{y}_{left})^2 +\sum_{y_j \in X_{right}} (y_j - \bar{y}_{right})^2 )</script>     (2-3)</p>
<p><strong>2.1.2 训练过程</strong></p>
<p><strong>RFR</strong>训练过程示意图如图2所示。</p>
<p>&lt;&gt;<img src="https://pic4.zhimg.com/80/v2-8123d3a064ff3e1ec60dec3fcd6c895b_hd.jpg" alt="img"></p>
<center>图 2. RFR训练示意图</center>

<p>说明：</p>
<ul>
<li>bootstrap[1]是对输入训练样本集合 \(D\)进行<strong>N</strong>次放回重复抽样得到样本集合 \(D_{b}\)(<strong>N</strong>一般等于\(D\) 的大小[2]，具体可以查看<em>sklearn</em>的实现<a href="http://link.zhihu.com/?target=https%3A//github.com/scikit-learn/scikit-learn/blob/55bf5d9/sklearn/ensemble/forest.py%23L101" target="_blank" rel="external">代码</a>)，这暗示了 <img src="https://www.zhihu.com/equation?tex=D" alt="D"> 中的某些样本会在 \(D_{b}\) 中重复出现多次。在对 \(D\) 进行bootstrap采样时，\(D\) 中每个样本会以<strong>63.3%</strong>的概率被抽中。为什么是<strong>63.3%</strong>？考虑N次放回重复抽样，每次抽样每个样本被选中的概率为 \(\frac{1}{N}\)，进行N次抽样，被选中概率为：</li>
</ul>
<p><script type="math/tex">(1- (1- \frac{1}{N})^N) =>(1- \lim_{N->\infty}{(1-\frac{1}{N})^N}) = (1- \frac{1}{e}) \simeq 0.633</script>      (2-4)</p>
<p>   具体可以参考[3-4]。</p>
<ul>
<li>subsample会根据输入参数<em>sample_sz</em>的大小从 \(D_{b}\)中采样<em>sample_sz</em>个样本组成subsample样本集合\(D_{bs}\)(在<em>sklearn</em>中，subsample大小就是输入训练样本集合 \(D\) 的大小N，不可以通过输入参数指定subsample的大小，详见<a href="http://link.zhihu.com/?target=https%3A//github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/forest.py%23L102" target="_blank" rel="external">这里</a>)。</li>
</ul>
<ul>
<li>为什么要进行bootstrap? 集成学习（包括随机森林）的目的是为了使用多个不同的子模型来增加最终模型预测结果的鲁棒性和稳定性(即减小方差)，如果多个子模型都采用同样的数据集训练，那么训练得出的子模型都是相同的，集成学习将变得没有意义，为了能从原输入训练数据集得到多个不同的数据集，需要使用bootstrap来从原数据集采样出不同的子数据集来训练子模型。</li>
</ul>
<p>图2中n个回归决策树的训练过程如图3所示。</p>
<p><img src="https://pic4.zhimg.com/80/v2-afdf542e209af2e0cd5ec5d9ea07956b_hd.jpg" alt="img">图 3. 回归决策树训练过程</p>
<p><strong>2.2 模型预测</strong></p>
<p>RFR的预测结果是由内部所有二叉决策树的预测结果取平均值得到的。二叉决策树的预测过程主要分为以下步骤：</p>
<ol>
<li>针对某一输入样本，从二叉决策树的根节点起，判断当前节点是否为叶子节点，如果是则返回叶子节点的预测值(即当前叶子中样本目标变量的平均值），如果不是则进入下一步；</li>
<li>根据当前节点的切分变量的和切分值，将样本中对应变量的值与节点的切分值对比。如果样本变量值小于等于当前节点切分值，则访问当前节点的左子节点；如果样本变量值大于当前节点切分值，则访问当前节点的右子节点；</li>
<li>循环步骤2，直到访问到叶子节点，并返回叶子节点的预测值。</li>
</ol>
<p><strong>2.3 计算feature importance</strong></p>
<p>特征的重要性表示特征对预测结果影响程度，某一特征重要性越大，表明该特征对预测结果的影响越大，重要性越小，表明该特征对预测结果越小。RFR中某一特征的重要性，是该特征在内部所有决策树重要性的平均值，而在决策树中，计算某一个特征的重要性可以采用以下三种方法：</p>
<p><strong>方法1</strong></p>
<p>步骤如下：</p>
<ul>
<li>使用训练数据训练模型；</li>
<li>计算训练数据在模型上依据某一metric的score，记为 \(s_o\) （在回归中，可以选用r2）；</li>
<li>遍历训练数据集中的每一个feature，每次在原训练数据集的基础上将对应的feature进行shuffle操作，然后使用模型得到shuffle后数据集的score，记为 \(s_i\) ，最后通过 \(\frac{s_o - s_i}{s_o}\)计算出第 \(i\) 个feature的重要性。</li>
</ul>
<p><strong>方法2</strong></p>
<p>如果一个feature很重要，那么从数据集中去除该feature后，模型的prediction error会增大；相反，如果一个feature不重要，那么从数据集中去除后，模型的prediction error不会变化太大。该方法的步骤如下：</p>
<ul>
<li>使用原数据集训练模型，并评估模型在训练数据集上的prediction error，记为 \(e_o\);</li>
<li>遍历训练数据集中的每一个feature，每次从原训练数据集的基础上去除该feature，然后使用得到数据集训练模型，最终计算出prediction error，记为 \(e_i\) ，最后通过 \(\frac{e_o - e_i}{e_o}\) 计算出第 \(i\) 个feature的重要性。</li>
</ul>
<p>该方法和方法1有点类似，但该方法在计算每个feature的重要性的时候都需要重新训练模型，会增加额外的计算量，在实际应用一般不采用该方法。</p>
<p><strong>方法3</strong></p>
<p>该方法是sklearn内部树模型计算feature重要性时采用的方法。即某一节点 \(k\) 的重要性为 </p>
<p><script type="math/tex">n_k = w_k * G_k - w_{left} * G_{left} - w_{right} * G_{right}</script>                (2-5)</p>
<p>其中， \(w_k,w_{left}, w_{right} \) 分别为节点 \(k\) 以及其左右子节点中训练样本个数与总训练样本数目的比例，  分为为节点  以及其左右子节点的不纯度。知道每一个节点的重要性之后，即可通过公式(2-6)得出某一feature的重要性。</p>
<p><script type="math/tex">f_i = \frac{\sum_{j \in nodes \space split \space on \space feature \space i } n_j}{\sum_{k \in all \space nodes}{ n_k}}</script>                                   (2-6)</p>
<p>为了使所有feature的重要性加起来等于1，需要每一feature的重要性进行normalization，即公式(2-7)</p>
<p><script type="math/tex">f_{ni} = \frac{f_i}{\sum_{j \in all \space features} f_j}</script>                                                    (2-7)</p>
<p>方法3在<em>sklearn</em>中的实现，请查看<a href="http://link.zhihu.com/?target=https%3A//github.com/scikit-learn/scikit-learn/blob/18cdaa69c14a5c84ab03fce4fb5dc6cd77619e35/sklearn/tree/_tree.pyx%23L1056" target="_blank" rel="external">_tree.pyx</a>。</p>
<p>在本文实现的RFR中，同时实现了<strong>方法1</strong>和<strong>方法3</strong>。</p>
<h2 id="三、回归随机森林实现"><a href="#三、回归随机森林实现" class="headerlink" title="三、回归随机森林实现"></a><strong>三、回归随机森林实现</strong></h2><p><strong>3.1 代码</strong></p>
<p>代码有点长，不想看的可以直接跳过。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div><div class="line">255</div><div class="line">256</div><div class="line">257</div><div class="line">258</div><div class="line">259</div><div class="line">260</div><div class="line">261</div><div class="line">262</div><div class="line">263</div><div class="line">264</div><div class="line">265</div><div class="line">266</div><div class="line">267</div><div class="line">268</div><div class="line">269</div><div class="line">270</div><div class="line">271</div><div class="line">272</div><div class="line">273</div><div class="line">274</div><div class="line">275</div><div class="line">276</div><div class="line">277</div><div class="line">278</div><div class="line">279</div><div class="line">280</div><div class="line">281</div><div class="line">282</div><div class="line">283</div><div class="line">284</div><div class="line">285</div><div class="line">286</div><div class="line">287</div><div class="line">288</div><div class="line">289</div><div class="line">290</div><div class="line">291</div><div class="line">292</div><div class="line">293</div><div class="line">294</div><div class="line">295</div><div class="line">296</div><div class="line">297</div><div class="line">298</div><div class="line">299</div><div class="line">300</div><div class="line">301</div><div class="line">302</div><div class="line">303</div><div class="line">304</div><div class="line">305</div><div class="line">306</div><div class="line">307</div><div class="line">308</div><div class="line">309</div><div class="line">310</div><div class="line">311</div><div class="line">312</div><div class="line">313</div><div class="line">314</div><div class="line">315</div><div class="line">316</div><div class="line">317</div><div class="line">318</div><div class="line">319</div><div class="line">320</div><div class="line">321</div><div class="line">322</div><div class="line">323</div><div class="line">324</div><div class="line">325</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool, cpu_count</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">RFRegressor</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="string">"""Random Forest Regressor</span></div><div class="line"><span class="string">    """</span></div><div class="line">    </div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_trees, sample_sz, min_leaf_sz=<span class="number">5</span>, n_jobs=None, max_depth=None)</span>:</span></div><div class="line">        self._n_trees = n_trees</div><div class="line">        self._sample_sz = sample_sz</div><div class="line">        self._min_leaf_sz = min_leaf_sz</div><div class="line">        self._n_jobs = n_jobs</div><div class="line">        self._max_depth = max_depth</div><div class="line">        self._trees = [self._create_tree() <span class="keyword">for</span> i <span class="keyword">in</span> range(self._n_trees)]</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_sample_data</span><span class="params">(self, bootstrap=True)</span>:</span></div><div class="line">        <span class="string">"""Generate training data for each underlying decision tree</span></div><div class="line"><span class="string">        </span></div><div class="line"><span class="string">        Parameters</span></div><div class="line"><span class="string">        ----------</span></div><div class="line"><span class="string">        bootstrap: boolean value, True/False</span></div><div class="line"><span class="string">            The default value is True, it would bootstrap sample from </span></div><div class="line"><span class="string">            input training data. If False, the exclusive sampling will</span></div><div class="line"><span class="string">            be performed.</span></div><div class="line"><span class="string">            </span></div><div class="line"><span class="string">        Returns</span></div><div class="line"><span class="string">        -------</span></div><div class="line"><span class="string">        idxs: array-like object</span></div><div class="line"><span class="string">            Return the indices of sampled data from input training data</span></div><div class="line"><span class="string">        """</span></div><div class="line">        <span class="keyword">if</span> bootstrap:</div><div class="line">            idxs = np.random.choice(len(self._X), self._sample_sz)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            idxs = np.random.permutation(len(self._X))[:self._sample_sz]</div><div class="line">        <span class="keyword">return</span> idxs</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_create_tree</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">"""Build decision treee</span></div><div class="line"><span class="string">        </span></div><div class="line"><span class="string">        Returns</span></div><div class="line"><span class="string">        -------</span></div><div class="line"><span class="string">        dtree : DTreeRegressor object</span></div><div class="line"><span class="string">        """</span></div><div class="line">        <span class="keyword">return</span> DTreeRegressor(self._min_leaf_sz, self._max_depth)</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_single_tree_fit</span><span class="params">(self, tree)</span>:</span></div><div class="line">        <span class="string">"""Fit the single underlying decision tree</span></div><div class="line"><span class="string">        </span></div><div class="line"><span class="string">        Parameters</span></div><div class="line"><span class="string">        ----------</span></div><div class="line"><span class="string">        tree : DTreeRegressor object</span></div><div class="line"><span class="string">        </span></div><div class="line"><span class="string">        Returns</span></div><div class="line"><span class="string">        -------</span></div><div class="line"><span class="string">        tree : DTreeRegressor object</span></div><div class="line"><span class="string">        """</span></div><div class="line">        sample_idxs = self._get_sample_data()</div><div class="line">        <span class="keyword">return</span> tree.fit(self._X.iloc[sample_idxs], self._y[sample_idxs])</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y)</span>:</span></div><div class="line">        <span class="string">"""Train a forest regressor of trees from the training set(x, y)</span></div><div class="line"><span class="string">        </span></div><div class="line"><span class="string">        Parameters</span></div><div class="line"><span class="string">        ----------</span></div><div class="line"><span class="string">        x : DataFrame,</span></div><div class="line"><span class="string">            The training input samples.</span></div><div class="line"><span class="string">            </span></div><div class="line"><span class="string">        y : Series or array-like object</span></div><div class="line"><span class="string">            The target values.</span></div><div class="line"><span class="string">        </span></div><div class="line"><span class="string">        """</span></div><div class="line">        self._X = x</div><div class="line">        self._y = y</div><div class="line">        <span class="keyword">if</span> self._n_jobs:</div><div class="line">            self._trees = self._parallel(self._trees, self._single_tree_fit, self._n_jobs)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">for</span> tree <span class="keyword">in</span> self._trees:</div><div class="line">                self._single_tree_fit(tree)</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="string">"""Predict target values using trained model</span></div><div class="line"><span class="string">        </span></div><div class="line"><span class="string">        Parameters</span></div><div class="line"><span class="string">        ---------</span></div><div class="line"><span class="string">        x : DataFrame or array-like object</span></div><div class="line"><span class="string">           input samples</span></div><div class="line"><span class="string">           </span></div><div class="line"><span class="string">        Returns</span></div><div class="line"><span class="string">        -------</span></div><div class="line"><span class="string">        ypreds : array-like object</span></div><div class="line"><span class="string">            predicted target values</span></div><div class="line"><span class="string">        """</span></div><div class="line">        all_tree_preds = np.stack([tree.predict(x) <span class="keyword">for</span> tree <span class="keyword">in</span> self._trees]) </div><div class="line">        <span class="keyword">return</span> np.mean(all_tree_preds, axis=<span class="number">0</span>)</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_parallel</span><span class="params">(self, trees, fn, n_jobs=<span class="number">1</span>)</span>:</span></div><div class="line">        <span class="string">"""Parallel jobs execution</span></div><div class="line"><span class="string">        </span></div><div class="line"><span class="string">        Parameters</span></div><div class="line"><span class="string">        ----------</span></div><div class="line"><span class="string">        trees : list-like object</span></div><div class="line"><span class="string">            a list-like object contains all underlying trees</span></div><div class="line"><span class="string">            </span></div><div class="line"><span class="string">        fn : function-like object</span></div><div class="line"><span class="string">            map function</span></div><div class="line"><span class="string">        </span></div><div class="line"><span class="string">        n_jobs : integer</span></div><div class="line"><span class="string">            The number of jobs.</span></div><div class="line"><span class="string">            </span></div><div class="line"><span class="string">        Returns</span></div><div class="line"><span class="string">        -------</span></div><div class="line"><span class="string">        result : list-like object</span></div><div class="line"><span class="string">            a list-like result object for each call of map function `fn`</span></div><div class="line"><span class="string">        """</span></div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            workers = cpu_count()</div><div class="line">        <span class="keyword">except</span> NotImplementedError:</div><div class="line">            workers = <span class="number">1</span></div><div class="line">        <span class="keyword">if</span> n_jobs:</div><div class="line">            workders = n_jobs</div><div class="line">        pool = Pool(processes=workers)</div><div class="line">        result = pool.map(fn, trees)</div><div class="line">        <span class="keyword">return</span> list(result)</div><div class="line">    </div><div class="line"><span class="meta">    @property</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feature_importances_</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">"""Calculate feature importance</span></div><div class="line"><span class="string">        </span></div><div class="line"><span class="string">        Returns</span></div><div class="line"><span class="string">        -------</span></div><div class="line"><span class="string">        self._feature_importances : array-like object</span></div><div class="line"><span class="string">            the importance score of each feature </span></div><div class="line"><span class="string">        """</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> hasattr(self, <span class="string">'_feature_importances'</span>):</div><div class="line">            norm_imp = np.zeros(len(self._X.columns))</div><div class="line">            <span class="keyword">for</span> tree <span class="keyword">in</span> self._trees:</div><div class="line">                t_imp = tree.calc_feature_importance()</div><div class="line">                norm_imp = norm_imp + t_imp / np.sum(t_imp)</div><div class="line">            self._feature_importances = norm_imp / self._n_trees</div><div class="line">        <span class="keyword">return</span> self._feature_importances</div><div class="line">       </div><div class="line"><span class="meta">    @property</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feature_importances_extra</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">"""Another method to calculate feature importance</span></div><div class="line"><span class="string">        """</span></div><div class="line">        norm_imp = np.zeros(len(self._X.columns))</div><div class="line">        <span class="keyword">for</span> tree <span class="keyword">in</span> self._trees:</div><div class="line">            t_imp = tree.calc_feature_importance_extra()</div><div class="line">            norm_imp = norm_imp + t_imp / np.sum(t_imp)</div><div class="line">        norm_imp = norm_imp / self._n_trees</div><div class="line">        imp = pd.DataFrame(&#123;<span class="string">'col'</span>:self._X.columns, <span class="string">'imp'</span>:norm_imp&#125;).sort_values(<span class="string">'imp'</span>, ascending=<span class="keyword">False</span>)</div><div class="line">        <span class="keyword">return</span> imp</div><div class="line">    </div><div class="line">    </div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DTreeRegressor</span><span class="params">(object)</span>:</span></div><div class="line">    </div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, min_leaf_sz, max_depth=None)</span>:</span></div><div class="line">        self._min_leaf_sz = min_leaf_sz</div><div class="line">        self._split_point = <span class="number">0</span></div><div class="line">        self._split_col_idx = <span class="number">0</span></div><div class="line">        self._score = float(<span class="string">'inf'</span>)</div><div class="line">        self._sample_sz = <span class="number">0</span></div><div class="line">        self._left_child_tree = <span class="keyword">None</span></div><div class="line">        self._right_child_tree = <span class="keyword">None</span></div><div class="line">        self._feature_importances = []</div><div class="line">        self._node_importance= <span class="number">0</span></div><div class="line">        <span class="keyword">if</span> max_depth <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            max_depth = max_depth - <span class="number">1</span></div><div class="line">        self._max_depth = max_depth</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y)</span>:</span></div><div class="line">        self._X = x</div><div class="line">        self._y = y</div><div class="line">        self._col_names = self._X.columns</div><div class="line">        self._feature_importances = np.zeros(len(self._col_names))</div><div class="line">        self._sample_sz = len(self._X)</div><div class="line">        self._val = np.mean(self._y)</div><div class="line">        <span class="keyword">if</span> self._max_depth <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> self._max_depth &lt; <span class="number">2</span>:</div><div class="line">            <span class="keyword">return</span> self</div><div class="line">        self._find_best_split()</div><div class="line">        <span class="keyword">return</span> self</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_calc_mse_inpurity</span><span class="params">(self, y_squared_sum, y_sum, n_y)</span>:</span></div><div class="line">        <span class="string">"""Calculate Mean Squared Error impurity</span></div><div class="line"><span class="string">        </span></div><div class="line"><span class="string">        This is just the recursive version for calculating variance</span></div><div class="line"><span class="string">        </span></div><div class="line"><span class="string">        Parameters</span></div><div class="line"><span class="string">        ----------</span></div><div class="line"><span class="string">        y_squared_sum: float or int , the sum  of y squared </span></div><div class="line"><span class="string">        </span></div><div class="line"><span class="string">        y_sum: float or int , the sum of y value</span></div><div class="line"><span class="string">        </span></div><div class="line"><span class="string">        n_y: int, the number of samples</span></div><div class="line"><span class="string">        </span></div><div class="line"><span class="string">        </span></div><div class="line"><span class="string">        Returns</span></div><div class="line"><span class="string">        -------</span></div><div class="line"><span class="string">        </span></div><div class="line"><span class="string">        """</span></div><div class="line">        dev = (y_squared_sum / n_y) - (y_sum / n_y) ** <span class="number">2</span></div><div class="line">        <span class="keyword">return</span> dev</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_find_best_split</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">for</span> col_idx <span class="keyword">in</span> range(len(self._col_names)):</div><div class="line">            self._find_col_best_split_point(col_idx)</div><div class="line">            </div><div class="line">        self._feature_importances[self._split_col_idx] = self._node_importance</div><div class="line">        </div><div class="line">        <span class="keyword">if</span> self.is_leaf:</div><div class="line">            <span class="keyword">return</span> </div><div class="line">        </div><div class="line">        left_child_sample_idxs = np.nonzero(self.split_col &lt;= self.split_point)[<span class="number">0</span>]</div><div class="line">        right_child_sample_idxs = np.nonzero(self.split_col &gt; self.split_point)[<span class="number">0</span>]</div><div class="line">        </div><div class="line">        self._left_child_tree = (DTreeRegressor(self._min_leaf_sz, self._max_depth)</div><div class="line">                                 .fit(self._X.iloc[left_child_sample_idxs], self._y[left_child_sample_idxs]))</div><div class="line">        self._right_child_tree = (DTreeRegressor(self._min_leaf_sz, self._max_depth)</div><div class="line">                                  .fit(self._X.iloc[right_child_sample_idxs], self._y[right_child_sample_idxs]))</div><div class="line">            </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_find_col_best_split_point</span><span class="params">(self, col_idx)</span>:</span></div><div class="line">        x_col = self._X.values[:, col_idx]</div><div class="line">        sorted_idxs = np.argsort(x_col)</div><div class="line">        sorted_x_col = x_col[sorted_idxs]</div><div class="line">        sorted_y =  self._y[sorted_idxs]</div><div class="line">        </div><div class="line">        lchild_n_samples = <span class="number">0</span></div><div class="line">        lchild_y_sum  = <span class="number">0.0</span></div><div class="line">        lchild_y_square_sum = <span class="number">0.0</span></div><div class="line"></div><div class="line">        rchild_n_samples = self._sample_sz</div><div class="line">        rchild_y_sum = sorted_y.sum()</div><div class="line">        rchild_y_square_sum = (sorted_y ** <span class="number">2</span>).sum()</div><div class="line">        </div><div class="line">        node_y_sum = rchild_y_sum</div><div class="line">        node_y_square_sum = rchild_y_square_sum </div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, self._sample_sz - self._min_leaf_sz):</div><div class="line">            xi, yi = sorted_x_col[i], sorted_y[i]</div><div class="line">            </div><div class="line">            rchild_n_samples -= <span class="number">1</span></div><div class="line">            rchild_y_sum -= yi</div><div class="line">            rchild_y_square_sum -= (yi ** <span class="number">2</span>)</div><div class="line">            </div><div class="line">            lchild_n_samples  +=  <span class="number">1</span></div><div class="line">            lchild_y_sum += yi</div><div class="line">            lchild_y_square_sum += (yi ** <span class="number">2</span>)</div><div class="line">            </div><div class="line">            <span class="keyword">if</span> i &lt; self._min_leaf_sz  <span class="keyword">or</span> xi == sorted_x_col[i+<span class="number">1</span>]:</div><div class="line">                <span class="keyword">continue</span></div><div class="line">            </div><div class="line">            lchild_impurity = self._calc_mse_inpurity(lchild_y_square_sum,</div><div class="line">                                                      lchild_y_sum, lchild_n_samples)</div><div class="line">            rchild_impurity = self._calc_mse_inpurity(rchild_y_square_sum,</div><div class="line">                                                      rchild_y_sum, rchild_n_samples)</div><div class="line">            split_score  = (lchild_n_samples * lchild_impurity </div><div class="line">                            + rchild_n_samples * rchild_impurity) / self._sample_sz</div><div class="line">            </div><div class="line">            <span class="keyword">if</span> split_score &lt; self._score:</div><div class="line">                self._score = split_score</div><div class="line">                self._split_point = xi</div><div class="line">                self._split_col_idx = col_idx</div><div class="line">                self._node_importance = (self._sample_sz </div><div class="line">                    * (self._calc_mse_inpurity(node_y_square_sum, node_y_sum, self._sample_sz) </div><div class="line">                    - split_score))</div><div class="line">                    </div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="keyword">if</span> type(x) == pd.DataFrame:</div><div class="line">            x = x.values</div><div class="line">        <span class="keyword">return</span> np.array([self._predict_row(row) <span class="keyword">for</span> row <span class="keyword">in</span> x])</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_predict_row</span><span class="params">(self, row)</span>:</span></div><div class="line">        <span class="keyword">if</span> self.is_leaf:</div><div class="line">            <span class="keyword">return</span> self._val</div><div class="line">        t = (self._left_child_tree <span class="keyword">if</span> row[self._split_col_idx] </div><div class="line">             &lt;= self.split_point <span class="keyword">else</span> self._right_child_tree)</div><div class="line">        <span class="keyword">return</span> t._predict_row(row)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></div><div class="line">        pr =  <span class="string">f'sample: <span class="subst">&#123;self._sample_sz&#125;</span>, value: <span class="subst">&#123;self._val&#125;</span>\r\n'</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.is_leaf:</div><div class="line">            pr += <span class="string">f'split column: <span class="subst">&#123;self.split_name&#125;</span>, \</span></div><div class="line"><span class="string">                split point: <span class="subst">&#123;self.split_point&#125;</span>, score: <span class="subst">&#123;self._score&#125;</span> '</span></div><div class="line">        <span class="keyword">return</span> pr</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_feature_importance</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">if</span> self.is_leaf:</div><div class="line">            <span class="keyword">return</span> self._feature_importances</div><div class="line">        <span class="keyword">return</span> (self._feature_importances </div><div class="line">                + self._left_child_tree.calc_feature_importance()</div><div class="line">                + self._right_child_tree.calc_feature_importance()</div><div class="line">               )</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_feature_importance_extra</span><span class="params">(self)</span>:</span></div><div class="line">        imp = []</div><div class="line">        o_preds = self.predict(self._X.values)</div><div class="line">        o_r2 = metrics.r2_score(self._y, o_preds)</div><div class="line">        <span class="keyword">for</span> col <span class="keyword">in</span> self._col_names:</div><div class="line">            tmp_x = self._X.copy()</div><div class="line">            shuffle_col = tmp_x[col].values</div><div class="line">            np.random.shuffle(shuffle_col)</div><div class="line">            tmp_x.loc[:,col] = shuffle_col</div><div class="line">            tmp_preds = self.predict(tmp_x.values)</div><div class="line">            tmp_r2 = metrics.r2_score(self._y, tmp_preds)</div><div class="line">            imp.append((o_r2 - tmp_r2))</div><div class="line">        imp = imp / np.sum(imp)</div><div class="line">        <span class="keyword">return</span> imp</div><div class="line">    </div><div class="line"><span class="meta">    @property</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">split_name</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> self._col_names[self._split_col_idx]</div><div class="line">    </div><div class="line"><span class="meta">    @property</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">split_col</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> self._X.iloc[:, self._split_col_idx]</div><div class="line"></div><div class="line"><span class="meta">    @property</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_leaf</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> self._score == float(<span class="string">'inf'</span>)</div><div class="line">    </div><div class="line"><span class="meta">    @property</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">split_point</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> self._split_point</div></pre></td></tr></table></figure>
<p><strong>3.2 测试</strong></p>
<p><strong>训练与预测：</strong></p>
<p>&lt;&gt;<img src="https://pic4.zhimg.com/80/v2-406efb4b5b5a3f69b16f36906636c5b3_hd.jpg" alt="img"></p>
<p>使用<strong>*sklearn*</strong>内部的<strong>*RandomForestRegressor*</strong>的结果：</p>
<p>&lt;&gt;<img src="https://pic2.zhimg.com/80/v2-cf23c1651fd18b722a36cee8aa3100cd_hd.jpg" alt="img"></p>
<p>需要注意的是，上面两次训练的样本不同，所以和实际情况有点出入，但大体趋势是对。</p>
<p><strong>feature importance:</strong></p>
<p>RFRegressor采用<strong>方法3</strong>计算的feature重要性：</p>
<p>&lt;&gt;<img src="https://pic2.zhimg.com/80/v2-3b36ab778879f5d834b34d9ce00b5759_hd.jpg" alt="img"></p>
<p>RandomForestRegressor计算的feature重要性：</p>
<p>&lt;;<img src="https://pic4.zhimg.com/80/v2-9b7b78ebcb92e6496a80d66f1ddf3dcb_hd.jpg" alt="img"></p>
<p>RFRegressor采用<strong>方法1</strong>计算的feature重要性：</p>
<p>&lt;&gt;<img src="https://pic1.zhimg.com/80/v2-8661db1ed6e8b314ec77fed8d082aaa0_hd.jpg" alt="img"></p>
<h2 id="参考与荐读："><a href="#参考与荐读：" class="headerlink" title="参考与荐读："></a><strong>参考与荐读：</strong></h2></div></article></div></main><footer><div class="paginator"><a href="/2018/12/13/feature-selection-tool-feature-selector/" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2019 <a href="https://free-free.github.io">infinite.ft</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>