<!DOCTYPE html><html lang="en,zh-cn,default"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Feature Selection Tool Feature-Selector · Infinite.ft的博客</title><meta name="description" content="Feature Selection Tool Feature-Selector - infinite.ft"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://free-free.github.io/atom.xml" title="Infinite.ft的博客"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">HOME</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/categories/math" target="_self" class="nav-list-link">MATH</a></li><li class="nav-list-item"><a href="/categories/tech" target="_self" class="nav-list-link">TECH</a></li><li class="nav-list-item"><a href="/categories/mylife" target="_self" class="nav-list-link">NOTES</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Feature Selection Tool Feature-Selector</h1><div class="post-info">Dec 13, 2018</div><div class="post-content"><h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a><strong>1. 前言</strong></h2><p>上一篇文章简要介绍了简谈ML模型特征选取的方法[6]，并使用sklearn提供的特征选择算法进行了demo性的演示，这篇文章主要介绍一个基础的特征选择工具：feature-selector[7]。feature-selector是由Feature Labs的一名数据科学家williamkoehrsen写的特征选择库。feature-selector主要对以下类型的特征进行选择：</p>
<ul>
<li>具有高missing-values百分比的特征</li>
<li>具有高相关性的特征</li>
<li>对模型预测结果无贡献的特征（即zero importance）</li>
<li>对模型预测结果只有很小贡献的特征（即low importance）</li>
<li>具有单个值的特征（即数据集中该特征取值的集合只有一个元素）</li>
</ul>
<p>从上面可以看出feature-selector确实是非常基础的特征选择工具，正因为非常的基础，所以才非常的常用（这也是为什么williamkoehrsen要写这个特征选择库的原因），在拿到一个数据集的时候，往往都需要将上述类型的特征从数据集中剔除掉。针对上面五种类型的特征，feature-selector分别提供以下五个函数来对此处理：</p>
<ul>
<li>identify_missing(*)</li>
<li>identify_collinear(*)</li>
<li>identify_zero_importance(*)</li>
<li>identify_low_importance(*)</li>
<li>identify_single_unique(*)</li>
</ul>
<p>接下来看看feature-selector具体使用方法。</p>
<h2 id="2-数据集准备"><a href="#2-数据集准备" class="headerlink" title="2. 数据集准备"></a><strong>2. 数据集准备</strong></h2><p>在这里使用kaggle上<a href="http://link.zhihu.com/?target=https%3A//www.kaggle.com/c/home-credit-default-risk/data" target="_blank" rel="external">Home Credit Default Risk Competition</a>的训练数据集。原训练数据集稍微有点大，30+万行(150+MB)，pandas导入数据都花了一点时间，为此我从原数据集中采样了1万+行数据作为此次练习的数据集，下载请戳<a href="http://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1G7eB09RVMd6hdIQbdOsxXw" target="_blank" rel="external">这里</a>，下载原数据集请戳<a href="http://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1K1vMVIBTsyighRxSXsk7Kw" target="_blank" rel="external">这里</a>。数据集采样代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">data = pd.read_csv(<span class="string">'./appliation_train.csv'</span>)</div><div class="line"><span class="comment"># 从原数据中采样5%的数据</span></div><div class="line">sample = data.sample(frac=<span class="number">0.05</span>)</div><div class="line"><span class="comment"># 重新创建索引</span></div><div class="line">sample.reset_index(drop=<span class="keyword">True</span>)</div><div class="line"><span class="comment"># 将采样数据存到'application_train_sample.csv'文件中</span></div><div class="line">sample.to_csv(<span class="string">'./application_train_sample.csv'</span>)</div></pre></td></tr></table></figure>
<h2 id="3-feature-selector用法"><a href="#3-feature-selector用法" class="headerlink" title="3.feature-selector用法"></a><strong>3.feature-selector用法</strong></h2><p><strong>3.1 导入数据并创建feaure-selector实例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas  <span class="keyword">as</span> pd</div><div class="line"><span class="comment"># 注意：</span></div><div class="line"><span class="comment"># 作者并没有把feature-selector发布到pypi上，所以不能使用pip和conda进行安装，只能手动</span></div><div class="line"><span class="comment"># 从github下载下来，然后把feature_selector.py文件放到当前工作目录，然后再进行import操作。</span></div><div class="line"><span class="keyword">from</span> feature_selector <span class="keyword">import</span> FeatureSelector</div><div class="line"></div><div class="line">data = pd.read_csv(<span class="string">'./application_train_sample.csv'</span>, index_col=<span class="number">0</span>)</div><div class="line"><span class="comment"># 数据集中TARGET字段为对应样本的label</span></div><div class="line">train_labels = data.TARGET</div><div class="line"><span class="comment"># 获取all features</span></div><div class="line">train_features = data.drop(columns=<span class="string">'TARGET'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 创建 feature-selector 实例，并传入features 和labels</span></div><div class="line">fs = FeatureSelector(data=train_features, labels=train_labels)</div></pre></td></tr></table></figure>
<p><strong>3.2 特征选取方法</strong></p>
<p><strong>（1）identify_missing</strong></p>
<p>该方法用于选择missing value 百分比大于指定值(通过missing_threshold指定百分比)的feature。<strong>该方法能应用于监督学习和非监督学习的特征选择。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 选择出missing value 百分比大于60%的特征</span></div><div class="line">fs.identify_missing(missing_threshold=<span class="number">0.6</span>)</div><div class="line"></div><div class="line"><span class="comment"># 查看选择出的特征</span></div><div class="line">fs.ops[<span class="string">'missing'</span>]</div><div class="line"></div><div class="line"><span class="comment"># 绘制所有特征missing value百分比的直方图</span></div><div class="line">fs.plot_missing()</div></pre></td></tr></table></figure>
<p>&lt;&gt;<img src="https://pic1.zhimg.com/80/v2-8bd9ed990b71e124eb7ab28a85ea48ec_hd.jpg" alt="img"></p>
<center>图1. 所有特征missing value百分比的直方图</center>

<p>该方法内部使用pandas 统计数据集中所有feature的missing value 的百分比，然后选择出百分比大于阈值的特征，详见<a href="http://link.zhihu.com/?target=https%3A//github.com/WillKoehrsen/feature-selector/blob/master/feature_selector/feature_selector.py%23L114-L136" target="_blank" rel="external">feature-selector.py的114-136行</a>。</p>
<h2 id="2-identify-collinear"><a href="#2-identify-collinear" class="headerlink" title="(2) identify_collinear"></a><strong>(2) identify_collinear</strong></h2><p>该方法用于选择相关性大于指定值(通过correlation_threshold指定值)的feature。<strong>该方法同样适用于监督学习和非监督学习。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 不对feature进行one-hot encoding（默认为False）, 然后选择出相关性大于98%的feature, </span></div><div class="line">fs.identify_collinear(correlation_threshold=<span class="number">0.98</span>, one_hot=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># 查看选择的feature</span></div><div class="line">fs.ops[<span class="string">'collinear'</span>]</div><div class="line"></div><div class="line"><span class="comment"># 绘制选择的特征的相关性heatmap</span></div><div class="line">fs.plot_collinear()</div><div class="line"></div><div class="line"><span class="comment"># 绘制所有特征的相关性heatmap</span></div></pre></td></tr></table></figure>
<p>&lt;&gt;<img src="https://pic2.zhimg.com/80/v2-bb3a7511d644a0744a4ce77e4980fe45_hd.jpg" alt="img"></p>
<center>图2. 选择的特征的相关矩阵图</center>

<p>&gt;<img src="https://pic2.zhimg.com/80/v2-110c6c6e652a7e4424681df873fbc13d_hd.jpg" alt="img"></p>
<center>图3. 所有特征相关矩阵图</center>

<p>该方法内部主要执行步骤如下：</p>
<ol>
<li>根据参数’one_hot’对数据集特征进行one-hot encoding（调用pd.get_dummies方法）。如果’one_hot=True’则对特征将进行one-hot encoding，并将编码的特征与原数据集整合起来组成新的数据集，如果’one_hot=False’则什么不做，进入下一步；</li>
<li>计算步骤1得出数据集的相关矩阵 <img src="https://www.zhihu.com/equation?tex=C" alt="C"> (通过DataFrame.corr()，注意 <img src="https://www.zhihu.com/equation?tex=C" alt="C"> 也为一个DateFrame)，并取相关矩阵的上三角部分得到 <img src="https://www.zhihu.com/equation?tex=C_%7Bupper%7D" alt="C_{upper}"> ；</li>
<li>遍历 <img src="https://www.zhihu.com/equation?tex=C_%7Bupper%7D" alt="C_{upper}"> 的每一列(即每一个特征)，如果该列的任何一个相关值大于correlation_threshold，则取出该列，并放到一个列表中（该列表中的feature，即具有high 相关性的特征，之后会从数据集去除）；</li>
<li>到这一步，做什么呢？回到源码看一波就知道了；</li>
</ol>
<p>具体请见<a href="http://link.zhihu.com/?target=https%3A//github.com/WillKoehrsen/feature-selector/blob/master/feature_selector/feature_selector.py%23L157-L227" target="_blank" rel="external">feature-selector.py的157-227行</a>。</p>
<p><strong>(3) identify_zero_importance</strong></p>
<p>该方法用于选择对模型预测结果毫无贡献的feature(即zero importance，从数据集中去除或者保留该feature对模型的结果不会有任何影响)<strong>。该方法以及之后的identify_low_importance都只适用于监督学习</strong>(即需要label,这也是为什么实例化feature-selector时需要传入labels参数的原因）。feature-selector通过用数据集训练一个梯度提升机(Gradient Boosting machine, GBM)，然后由GBM得到每一个feature的重要性分数，对所有特征的重要性分数进行归一化处理，选择出重要性分数等于零的feature<strong>。</strong></p>
<p>为了使计算得到的feature重要性分数具有很小的方差，identify_zero_importance内部会对GBM训练多次，取多次训练的平均值，得到最终的feature重要性分数。同时为了防止过拟合，identify_zero_importance内部从数据集中抽取一部分作为验证集，在训练GBM的时候，计算GBM在验证集上的某一metric，当metric满足一定条件时，停止GBM的训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 选择zero importance的feature,</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment"># 参数说明：</span></div><div class="line"><span class="comment">#          task: 'classification' / 'regression', 如果数据的模型是分类模型选择'classificaiton',</span></div><div class="line"><span class="comment">#                否则选择'regression'</span></div><div class="line"><span class="comment">#          eval_metric: 判断提前停止的metric. for example, 'auc' for classification, and 'l2' for regression problem</span></div><div class="line"><span class="comment">#          n_iteration: 训练的次数</span></div><div class="line"><span class="comment">#          early_stopping: True/False, 是否需要提前停止</span></div><div class="line">fs.identify_zero_importance(task=<span class="string">'classification'</span>,</div><div class="line">                                             eval_metric=<span class="string">'auc'</span>,</div><div class="line">                                             n_iteration=<span class="number">10</span>,</div><div class="line">                                             early_stopping=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 查看选择出的zero importance feature</span></div><div class="line">fs.ops[<span class="string">'zero_importance'</span>]</div><div class="line"></div><div class="line"><span class="comment"># 绘制feature importance 关系图</span></div><div class="line"><span class="comment"># 参数说明：</span></div><div class="line"><span class="comment">#          plot_n: 指定绘制前plot_n个最重要的feature的归一化importance条形图，如图4所示</span></div><div class="line"><span class="comment">#          threshold: 指定importance分数累积和的阈值，用于指定图4中的蓝色虚线.</span></div><div class="line"><span class="comment">#              蓝色虚线指定了importance累积和达到threshold时，所需要的feature个数。</span></div><div class="line"><span class="comment">#              注意：在计算importance累积和之前，对feature列表安装feature importance的大小</span></div><div class="line"><span class="comment">#                   进行了降序排序</span></div><div class="line"><span class="comment"># </span></div><div class="line"><span class="comment">#      </span></div><div class="line">fs.plot_feature_importances(threshold=<span class="number">0.99</span>, plot_n=<span class="number">12</span>)</div></pre></td></tr></table></figure>
<p><img src="https://pic3.zhimg.com/80/v2-3e547283d68d836473c08c81bf1eae76_hd.jpg" alt="img"></p>
<center>图4. 前12个最重要的feature归一化后的importance分数的条形图</center>

<p>&lt;&gt;<img src="https://pic2.zhimg.com/80/v2-9564e37b49495b7766b1be4657a119f1_hd.jpg" alt="img"></p>
<center>图5. feature 个数与feature importance累积和的关系图</center>

<p><strong>需要注意GBM训练过程是随机的，所以每次运行identify_zero_importance得到feature importance分数都会发生变化，但按照importance排序之后，至少前几个最重要的feature顺序不会变化。</strong></p>
<p>该方法内部主要执行了以下步骤：</p>
<ol>
<li>对各个feature进行one-hot encoding，然后将one-hot encoding的feature和原数据集合并成新的数据集(使用pd.get_dummies完成)；</li>
<li>根据task的取值，实例化lightgbm.LGBMClassifier或者lightgbm.LGBMRegressor model；</li>
<li>根据early_stopping的取值选择是否需要提前停止训练，并向model.fit传入相应的参数，然后开始训练model；</li>
<li>根据model得到该次训练的feature importance；</li>
<li>执行n_iterations次步骤1-4；</li>
<li>取多次训练的feature importance的平均值，得到最终的feature importance；</li>
<li>选择出feature importance等于0的feature；</li>
<li>到这一步，主要步骤完成了，其他部分请查看源码。</li>
</ol>
<p>具体请见<a href="http://link.zhihu.com/?target=https%3A//github.com/WillKoehrsen/feature-selector/blob/master/feature_selector/feature_selector.py%23L229-L342" target="_blank" rel="external">feature-selector.py的229-342行</a>。</p>
<p><strong>(4) identify_low_importance</strong></p>
<p>该方法是使用identify_zero_importance计算的结果，选择出对importance累积和达到指定阈值没有贡献的feature（这样说有点拗口），即图5中蓝色虚线之后的feature。<strong>该方法只适用于监督学习。</strong>identify_low_importance有点类似于PCA中留下主要分量去除不重要的分量[1]。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 选择出对importance累积和达到99%没有贡献的feature</span></div><div class="line">fs.identify_low_importance(cumulative_importance=<span class="number">0.99</span>)</div><div class="line"></div><div class="line"><span class="comment"># 查看选择出的feature</span></div><div class="line">fs.ops[<span class="string">'low_importance'</span>]</div></pre></td></tr></table></figure>
<p>该方法选择出的feature其实包含了zero importance的feature。内部实现没什么可说的，具体请见<a href="http://link.zhihu.com/?target=https%3A//github.com/WillKoehrsen/feature-selector/blob/master/feature_selector/feature_selector.py%23L344-L378" target="_blank" rel="external">feature-selector.py的344-378行</a>。</p>
<p><strong>(5) identify_single_unique</strong></p>
<p>该方法用于选择只有单个取值的feature，单个值的feature的方差为0，对于模型的训练不会有任何作用（从信息熵的角度看，该feature的熵为0）。<strong>该方法可应用于监督学习和非监督学习。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 选择出只有单个值的feature</span></div><div class="line">fs.identify_single_unique()</div><div class="line"></div><div class="line"><span class="comment"># 查看选择出的feature</span></div><div class="line">fs.ops[<span class="string">'single_unique'</span>]</div><div class="line"></div><div class="line"><span class="comment">#绘制所有feature unique value的直方图</span></div><div class="line">fs.plot_unique()</div></pre></td></tr></table></figure>
<p>&lt;&gt;<img src="https://pic3.zhimg.com/80/v2-c3bc955fd6114742094b68c6fbbd949a_hd.jpg" alt="img"></p>
<center>图6. 所有feature unique value的直方图</center>

<p>该方法内部的内部实现很简单，只是通过DataFrame.nunique方法统计了每个feature取值的个数，然后选择出nunique==1等于1的feature。具体请见<a href="http://link.zhihu.com/?target=https%3A//github.com/WillKoehrsen/feature-selector/blob/master/feature_selector/feature_selector.py%23L138-L155" target="_blank" rel="external">feature-selector.py的138-155行</a>。</p>
<p><strong>3.3 从数据集去除选择的特征</strong></p>
<p>3.2中介绍了feature-selector提供的特征选择方法，这些方法从数据集中识别了feature，但并没有从数据集中将这些feature去除。feature-selector中提供了remove方法将选择的特征从数据集中去除，并返回去除特征之后的数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 去除所有类型的特征</span></div><div class="line"><span class="comment">#    参数说明：</span></div><div class="line"><span class="comment">#       methods: </span></div><div class="line"><span class="comment">#               desc:  需要去除哪些类型的特征</span></div><div class="line"><span class="comment">#               type:  string / list-like object</span></div><div class="line"><span class="comment">#             values:  'all' 或者是 ['missing', 'single_unique', 'collinear', 'zero_importance', 'low_importance']</span></div><div class="line"><span class="comment">#                      中多个方法名的组合</span></div><div class="line"><span class="comment">#      keep_one_hot: </span></div><div class="line"><span class="comment">#              desc: 是否需要保留one-hot encoding的特征</span></div><div class="line"><span class="comment">#              type: boolean</span></div><div class="line"><span class="comment">#              values: True/False</span></div><div class="line"><span class="comment">#              default: True</span></div><div class="line">train_removed = fs.remove(methods = <span class="string">'all'</span>, keep_one_hot=<span class="keyword">False</span>)</div></pre></td></tr></table></figure>
<p>注意：调用remove函数的时候，必须先调用特征选择函数，即identify_*函数。</p>
<p>该方法的实现代码在<a href="http://link.zhihu.com/?target=https%3A//github.com/WillKoehrsen/feature-selector/blob/master/feature_selector/feature_selector.py%23L430-L510" target="_blank" rel="external">feature-selector.py的430-510行</a>。</p>
<p><strong>3.4 一次性选择所有类型的特征</strong></p>
<p>feature-selector除了能每次运行一个<strong>identify_*</strong>函数来选择一种类型特征外，还可以使用<strong>identify_all</strong>函数一次性选择5种类型的特征选。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 注意：</span></div><div class="line"><span class="comment"># 少了下面任何一个参数都会报错，raise ValueError</span></div><div class="line">fs.identify_all(selection_params = &#123;<span class="string">'missing_threshold'</span>: <span class="number">0.6</span>,    </div><div class="line">                                    <span class="string">'correlation_threshold'</span>: <span class="number">0.98</span>, </div><div class="line">                                    <span class="string">'task'</span>: <span class="string">'classification'</span>,    </div><div class="line">                                    <span class="string">'eval_metric'</span>: <span class="string">'auc'</span>, </div><div class="line">                                    <span class="string">'cumulative_importance'</span>: <span class="number">0.99</span>&#125;)</div></pre></td></tr></table></figure>
<h2 id="4-结尾"><a href="#4-结尾" class="headerlink" title="4. 结尾"></a><strong>4. 结尾</strong></h2><p><strong>feature-selector</strong>属于非常基础的特征选择工具，它提供了五种特征的选择函数，每个函数负责选择一种类型的特征。一般情况下，在对某一数据集构建模型之前，都需要考虑从数据集中去除这五种类型的特征，所以feature-selector帮你省去data-science生活中一部分重复性的代码工作。如果有兴趣和充足的时间，建议阅读一下feature-selector的代码，代码量很少，七百多行，相信看了之后对feature-selector各个函数的实现思路以及相应代码实现有一定认识，有心者还可以贡献一下自己的代码。</p>
<h2 id="参考与荐读："><a href="#参考与荐读：" class="headerlink" title="参考与荐读："></a><strong>参考与荐读：</strong></h2><p><a href="http://link.zhihu.com/?target=https%3A//towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60" target="_blank" rel="external">[1] PCA using Python (scikit-learn)</a></p>
<p><a href="http://link.zhihu.com/?target=https%3A//stats.stackexchange.com/questions/162162/relative-variable-importance-for-boosting" target="_blank" rel="external">[2] Relative variable importance for Boosting</a></p>
<p><a href="http://link.zhihu.com/?target=https%3A//machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/" target="_blank" rel="external">[3] Feature Importance and Feature Selection With XGBoost in Python</a></p>
<p><a href="http://link.zhihu.com/?target=https%3A//www.salford-systems.com/blog/dan-steinberg/what-is-the-variable-importance-measure" target="_blank" rel="external">[4] What is the Variable Importance Measure?</a></p>
<p><a href="http://link.zhihu.com/?target=https%3A//towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0" target="_blank" rel="external">[5] A Feature Selection Tool for Machine Learning in Python</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/48502371" target="_blank" rel="external">[6] 简谈ML模型特征选取的方法</a></p>
<p><a href="http://link.zhihu.com/?target=https%3A//github.com/WillKoehrsen/feature-selector" target="_blank" rel="external">[7] feature-selector Github地址</a></p>
</div></article></div></main><footer><div class="paginator"><a href="/2018/11/05/a-brief-introduction-to-feature-selection-methods/" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2019 <a href="https://free-free.github.io">infinite.ft</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>