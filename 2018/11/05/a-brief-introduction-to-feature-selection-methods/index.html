<!DOCTYPE html><html lang="en,zh-cn,default"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> A Brief Introduction to Feature Selection Methods · Infinite.ft的博客</title><meta name="description" content="A Brief Introduction to Feature Selection Methods - infinite.ft"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://free-free.github.io/atom.xml" title="Infinite.ft的博客"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">HOME</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/categories/math" target="_self" class="nav-list-link">MATH</a></li><li class="nav-list-item"><a href="/categories/tech" target="_self" class="nav-list-link">TECH</a></li><li class="nav-list-item"><a href="/categories/mylife" target="_self" class="nav-list-link">NOTES</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">A Brief Introduction to Feature Selection Methods</h1><div class="post-info">Nov 5, 2018</div><div class="post-content"><h3 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. <strong>前言</strong></h3><p>在对某一数据集构建ML模型时，往往需要先进行特征选择[15]，因为并不是所有特征能够提供足够多有用的信息，需要去除那些无关紧要的特征，留下主要的特征（有点类似于SVD分解[1-3]，留下主要分量，但只是类似）。特征选取的好坏直接影响模型的性能，好的特征选取能够加快模型的训练速度，改善模型的预测性能，同时使我们对数据背后的真正分布有一个更好的理解（模型越简单，越容易理解，遵循奥卡姆剃刀原则[4]是必要的）。在本文中，<strong>简要</strong>介绍三个特征选取的方法：Filter methods, Wapper Methods, Embedded Methods，并使用<strong>sklearn</strong>提供的算法演示一下特征选取的过程。</p>
<h3 id="2-特征选取方法"><a href="#2-特征选取方法" class="headerlink" title="2. 特征选取方法"></a><strong>2. 特征选取方法</strong></h3><p><strong>(1) Filter Methods</strong></p>
<p>基于Filter特征选择方法是通过一定的统计度量方法来计算出数据集中的每一个feature的score，然后根据score大小来选取或者剔除数据集中的特征，进而达到特征选取的目的。该方法即可应用于独立的feature，也可针对于有关联的feature。常见的方法卡方测试[5-6]，信息增益[7-9]，Gini[10-11] 和相关系数[12-13]。信息增益和Gini在决策树中很常见，相关系数法在Kaggle的kernel中特征选取常见。</p>
<p><strong>(2) Wrapper Methods</strong></p>
<p>Wrapper Methods将特征选取问题视为搜索问题，即将数据集中特征进行组合，然后使用一个ML模型对各个特征组合进行评估，最后选取出最优的特征组合。常用的算法是Recursive Feature Elimination(RFE)[14]。</p>
<p><strong>(3) Embedded Methods</strong></p>
<p>Embedded Methods是一种比较新的特征选取方法，相比于前述两种在模型训练之前进行特征选取，Embedded Methods是在模型训练的时候隐式的进行特征选取（在这里隐式是相比前述两种直接进行特征筛选过程而言），最常用的莫过于正则化。使用正则化的例子有LASSO，Ridge Regression[16]。</p>
<p>图1 是从从Wikipedia下截取的关于各种特征选取方法的应用场景和相应算法[15].</p>
<p><img src="https://pic1.zhimg.com/v2-91b08827c024e4d494f09d1f23c7514c_r.jpg" alt=""></p>
<center>图1. 各种特征选取方法应用场景</center>

<p>上面简要介绍了一下关于特征选取的方法，接下来演示一下使用<strong>sklearn  </strong>提供的算法进行特征选取的过程。</p>
<h3 id="3-使用sklearn-进行特征选取"><a href="#3-使用sklearn-进行特征选取" class="headerlink" title="3. 使用sklearn 进行特征选取"></a><strong>3. 使用sklearn 进行特征选取</strong></h3><p><strong>（1） SelectKBest </strong></p>
<p>sklearn提供了<strong>SelectKBest</strong>方法，以根据不同的统计度量方法来选取特征。在这里演示使用卡方测试来对pima-indians-diabetes数据集的特征进行选取。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</div><div class="line"><span class="comment"># chi2适用于分类任务</span></div><div class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</div><div class="line"></div><div class="line">url = <span class="string">"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"</span></div><div class="line">names = [<span class="string">'preg'</span>, <span class="string">'plas'</span>, <span class="string">'pres'</span>, <span class="string">'skin'</span>, <span class="string">'test'</span>, <span class="string">'mass'</span>, <span class="string">'pedi'</span>, <span class="string">'age'</span>, <span class="string">'class'</span>]</div><div class="line">data = pd.read_csv(url, names=names, header=<span class="keyword">None</span>)</div><div class="line"></div><div class="line">narr = data.values</div><div class="line">X = narr[:, <span class="number">0</span>:<span class="number">8</span>]</div><div class="line">y = narr[:, <span class="number">8</span>]</div><div class="line"></div><div class="line"><span class="comment"># 使用chi2作为度量函数，选取前四个特征</span></div><div class="line">skb= SelectKBest(score_func=chi2, k=<span class="number">4</span>)</div><div class="line"></div><div class="line"><span class="string">"""</span></div><div class="line"><span class="string">score_func 的可选值有：</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">f_classif:  使用label和feature之间方差分析的F值作为选取准则，适用于分类</span></div><div class="line"><span class="string">mutual_info_classif: 使用离散target的互信息作为选取准则，适用于分类.</span></div><div class="line"><span class="string">chi2: 使用非负features的卡方测试作为选取准则，适用于分类.</span></div><div class="line"><span class="string">f_regression:  使用label和feature的F值作为选取准则，适用于回归.</span></div><div class="line"><span class="string">mutual_info_regression: 使用连续变量的互信息作为选取准则，适用于回归.</span></div><div class="line"><span class="string">SelectPercentile: Select features based on percentile of the highest scores.</span></div><div class="line"><span class="string">SelectFpr: Select features based on a false positive rate test.</span></div><div class="line"><span class="string">SelectFdr: Select features based on an estimated false discovery rate.</span></div><div class="line"><span class="string">SelectFwe: Select features based on family-wise error rate.</span></div><div class="line"><span class="string">GenericUnivariateSelect: Univariate feature selector with configurable mode.</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">"""</span></div><div class="line">fit =  skb.fit(X, y)</div><div class="line"><span class="comment"># 查看各个feature对应的score, score越高，该feature越重要</span></div><div class="line">print(fit.scores_)</div><div class="line"><span class="comment"># 获取筛选之后的features</span></div><div class="line">X_selected =  fit.transform(X)</div><div class="line"><span class="comment"># 执行到这一步之后，特征选取过程也就基本结束。</span></div></pre></td></tr></table></figure>
<p><strong>(2) RFE</strong></p>
<p><strong>sklearn </strong>提供了<strong>RFE</strong>作为Recursive Feature Elimination的实现。在使用<strong>RFE</strong>的过程中需要使用一个模型来对不同的特征组合进行评估，这里选用逻辑回归在对<strong>iris</strong>数据的特征进行选取，具体特征选取过程如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</div><div class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</div><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris </div><div class="line"></div><div class="line"></div><div class="line">iris = load_iris()</div><div class="line">X = iris.data</div><div class="line">y = iris.target</div><div class="line">logreg = LogisticRegression()</div><div class="line"><span class="comment"># 选取两个特征</span></div><div class="line">rfe = RFE(logreg, <span class="number">2</span>)</div><div class="line">fit = rfe.fit(X, y)</div><div class="line"></div><div class="line"><span class="comment"># fit.support_返回一个True/False 组成的list, 每个元素与每一个特征对应，元素为True时，</span></div><div class="line"><span class="comment"># 表示该特征被选取了</span></div><div class="line">print(fit.support_)</div><div class="line"><span class="comment"># 返回一个由integer组成的list，值为1表示对应的特征被选取了</span></div><div class="line">print(fit.ranking_)</div><div class="line"><span class="comment"># 打印出选出的特征</span></div><div class="line">print([iris.feature_names[index] <span class="keyword">for</span> index, ret <span class="keyword">in</span>  enumerate(fit.support_) <span class="keyword">if</span> ret])</div></pre></td></tr></table></figure>
<p><strong>(3) PCA</strong></p>
<p>Principal Component Analysis(PCA)作为一种降维方法也可以用于特征选择。PCA对数据找到一种新的表示方式(即线性代数里基的概念)，经过PCA变换之后，原有数据会发生发变化(维数变化，数据本身的变化)， 而前述的特征选取方法只是简单剔除不重要的特征，保留下来的feature并不会发生变化。关于PCA的资料很多，这里不再多嘴了，还是看看sklearn怎样使用PCA吧。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div><div class="line"></div><div class="line">url = <span class="string">"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"</span></div><div class="line">names = [<span class="string">'preg'</span>, <span class="string">'plas'</span>, <span class="string">'pres'</span>, <span class="string">'skin'</span>, <span class="string">'test'</span>, <span class="string">'mass'</span>, <span class="string">'pedi'</span>, <span class="string">'age'</span>, <span class="string">'class'</span>]</div><div class="line">data = pd.read_csv(url, names=names, header=<span class="keyword">None</span>)</div><div class="line"></div><div class="line">narr = data.values</div><div class="line">X = narr[:, <span class="number">0</span>:<span class="number">8</span>]</div><div class="line">y = narr[:, <span class="number">8</span>]</div><div class="line"></div><div class="line"><span class="comment"># 选取4个主要成分，其实PCA是特殊的SVD</span></div><div class="line">pca = PCA(n_components=<span class="number">4</span>)</div><div class="line">fit = pca.fit(X, y)</div><div class="line">print(<span class="string">"每个主成分的方差: %s"</span> % fit.explained_variance_ratio_)</div><div class="line"><span class="comment"># 打印出变换矩阵</span></div><div class="line">print(fit.components_)</div><div class="line"><span class="comment"># 得到PCA变换的之后数据</span></div><div class="line">print(fit.components_.dot(X.T).T)</div></pre></td></tr></table></figure>
<p><strong>(4) Feature Importance</strong></p>
<p>一个feature的重要性是通过计算该feature后移除前后，模型预测错误增加来衡量的[17]。如果一个feature很重要，那么在数据集去除该feature之后，模型的prediction error将会增加，如果该feature对模型的预测没有任何贡献，那么去掉feature之后，模型的prediction error不会变化太大，换句话说，该feature不重要，可以从数据集剔除。具体请参考[17]。</p>
<p>在sklearn中，可以用Random Forest 和Extra Trees来计算每一个feature的重要性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesClassifier</div><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris()</div><div class="line"></div><div class="line">iris = load_iris()</div><div class="line">X = iris.data</div><div class="line">y = iris.target</div><div class="line"></div><div class="line">tr = ExtraTreesClassifier()</div><div class="line">fit = tr.fit(X, y)</div><div class="line"><span class="comment"># 打印出每个feature的重要性score</span></div><div class="line">print(fit.feature_importances_)</div><div class="line"><span class="comment"># 打印出Top 2 important features</span></div><div class="line">d  = list(enumerate(fit.feature_importances_)</div><div class="line">d.sort(key = <span class="keyword">lambda</span> a:a[<span class="number">1</span>], reverse=<span class="keyword">True</span>)</div><div class="line">print([iris.feature_names[item[<span class="number">0</span>]] <span class="keyword">for</span> index, item <span class="keyword">in</span> enumerate(d) <span class="keyword">if</span> index &lt; <span class="number">2</span>])</div></pre></td></tr></table></figure>
<p>到这里算是结束了，上述代码只是demo性，要真正理解什么时候使用什么样的方法，还需要多实践才行，虽然ML是一门科学，但是多少还需要人为经验参与其中进行参数选择和调试。</p>
<p><strong>参考:</strong></p>
<p><a href="http://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%25E5%25A5%2587%25E5%25BC%2582%25E5%2580%25BC%25E5%2588%2586%25E8%25A7%25A3" target="_blank" rel="external">[1] 奇异值分解</a></p>
<p><a href="http://link.zhihu.com/?target=https%3A//blog.csdn.net/zhongkejingwang/article/details/43053513" target="_blank" rel="external">[2] 奇异值分解(SVD)原理详解及推导</a></p>
<p><a href="http://link.zhihu.com/?target=https%3A//www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html" target="_blank" rel="external">[3] 机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用</a></p>
<p><a href="http://link.zhihu.com/?target=https%3A//zh.wikipedia.org/zh-hans/%25E5%25A5%25A5%25E5%258D%25A1%25E5%25A7%2586%25E5%2589%2583%25E5%2588%2580" target="_blank" rel="external">[4] 奥卡姆剃刀原理</a></p>
<p><a href="http://link.zhihu.com/?target=https%3A//www.spss-tutorials.com/chi-square-independence-test/" target="_blank" rel="external">[5] Chi-Square Independence Test – What and Why?</a></p>
<p><a href="http://link.zhihu.com/?target=https%3A//www.statisticssolutions.com/using-chi-square-statistic-in-research/" target="_blank" rel="external">[6] Using Chi-Square Statistic in Research</a></p>
<p><a href="http://link.zhihu.com/?target=https%3A//homes.cs.washington.edu/%7Eshapiro/EE596/notes/InfoGain.pdf" target="_blank" rel="external">[7] Information Gain</a></p>
<p><a href="http://link.zhihu.com/?target=https%3A//medium.com/coinmonks/what-is-entropy-and-why-information-gain-is-matter-4e85d46d2f01" target="_blank" rel="external">[8] What is Information gain and why it is matter in Decision Tree?</a></p>
<p><a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Information_gain_in_decision_trees" target="_blank" rel="external">[9] Information gain in decision tree</a></p>
<p><a href="http://link.zhihu.com/?target=https%3A//www.devtalking.com/articles/machine-learning-16/" target="_blank" rel="external">[10] 机器学习笔记十六之基尼系数、CART</a></p>
<p><a href="http://link.zhihu.com/?target=https%3A//www.jianshu.com/p/abfec6ecf974" target="_blank" rel="external">[11] 信息熵 GINI系数</a></p>
<p><a href="http://link.zhihu.com/?target=https%3A//www.cs.waikato.ac.nz/%7Emhall/thesis.pdf" target="_blank" rel="external">[12] Correlation-based Feature Selection for Machine Learning</a></p>
<p><a href="http://link.zhihu.com/?target=https%3A//www.andrews.edu/%7Ecalkins/math/edrm611/edrm05.htm" target="_blank" rel="external">[13] Correlation Coefficients</a></p>
<p><a href="http://link.zhihu.com/?target=https%3A//topepo.github.io/caret/recursive-feature-elimination.html" target="_blank" rel="external">[14] Recursive Feature Elimination</a></p>
<p><a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Feature_selection" target="_blank" rel="external">[15] feature selection</a></p>
<p><a href="http://link.zhihu.com/?target=https%3A//www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/" target="_blank" rel="external">[16] A comprehensive beginners guide for Linear, Ridge and Lasso Regression</a></p>
<p><a href="http://link.zhihu.com/?target=https%3A//christophm.github.io/interpretable-ml-book/feature-importance.html" target="_blank" rel="external">[17] Feature Importance</a></p>
</div></article></div></main><footer><div class="paginator"><a href="/2018/09/17/independent-components-analysis/" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2019 <a href="https://free-free.github.io">infinite.ft</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>