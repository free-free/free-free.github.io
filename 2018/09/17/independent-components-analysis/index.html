<!DOCTYPE html><html lang="en,zh-cn,default"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Independent Components analysis(ICA) · Infinite.ft的博客</title><meta name="description" content="Independent Components analysis(ICA) - infinite.ft"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://free-free.github.io/atom.xml" title="Infinite.ft的博客"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">HOME</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/categories/math" target="_self" class="nav-list-link">MATH</a></li><li class="nav-list-item"><a href="/categories/tech" target="_self" class="nav-list-link">TECH</a></li><li class="nav-list-item"><a href="/categories/mylife" target="_self" class="nav-list-link">NOTES</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Independent Components analysis(ICA)</h1><div class="post-info">Sep 17, 2018</div><div class="post-content"><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Independent Components Analysis(ICA) is an approach to separate out the basis signals, given only  the observation data  without knowing the prior knowledge about the basis signals. Similar to PCA,  this find out a new basis in which to present data, however, their goal is way out different.  </p>
<p>Consider a heavily used example: “Cocktail Party Problem”. Here, n speakers are speaking simultaneously in a party room, the several microphones placed in the room record the different overlapping combinations of the n speakers’ voices. Using only the recordings of the several microphones, can we recover the n speakers’ original voices respectively? The answer is yes, ICA is desired to solve this problem separating out the n speakers’ original voices signal.</p>
<p>To make this more formally, suppose that there is some data $s \in R^n$ generated via n independent source, now using some kinds of devices , like sensors,  some observation data  $x$ is observed from  the source signals. From mathematical view, the above observing process would be defined as</p>
<script type="math/tex; mode=display">x = As</script><p>where $x$ is the observation data, $s$ is the source signal, and $A$ is a matrix called mixing matrix. Repeated observations generates a dataset $\{x^1, x^2, x^3, x^4,\dots, x^m \}$, where $m$ represents the observation times and $x^i$ represents the observation data vector, The purpose of ICA is to find out the source signal $s_i$ from the observation data  $x^i$ by solving out a matrix $W$ called unmixing matrix, which is, in essence, equal to the inverse of the mixing matrix $A$. Indeed, the main tricks of ICA is all about the way to get unmixing matrix $W$. So how to solve out(the term <code>estimate</code> should be more appropriate here) $W$ with only knowing $x$?.</p>
<p>Here suppose the distribution of each of the source $s_i$ is given by a density function $p(s_i)$, and that the joint distribution of all source $s=[s_1, s_2, s_3,\dots,s_n]^T$ is given by </p>
<script type="math/tex; mode=display">p(s) = \prod_{i=1}^{n}{p(s_i)}</script><p>Note that by writing the joint distribution as a product of the marginal PDF, it indicates that the n source $s_i$ are independent to each other. Cause $x=As,s = Wx$, the joint distribution of $x^i$ can be given by </p>
<script type="math/tex; mode=display">p(x^i) = \prod_{j=1}^{n}{p(w_j^Tx^i)}\cdot|W|</script><p>where $w_j^T$ is the jth row of unmixing matrix $W$, i.e.</p>
<script type="math/tex; mode=display">W = \begin{bmatrix} - w_1^T-  \\  \vdots  \\ - w_n^T - \end{bmatrix}</script><p>Now, the joint distribution of dataset $\{x^1,x^2,x^3,\dots,x^m\}$ is</p>
<script type="math/tex; mode=display">p(x) = \prod_{i=1}^{m}{\prod_{j=1}^{n}{p(w_j^Tx^i)\cdot|W|}}</script><p>Take logarithm of $p(x)$,  the joint distribution of dataset becomes</p>
<script type="math/tex; mode=display">P(x) = \sum_{i=1}^{m}\sum_{j=1}^{n}\log p(w_j^Tx^i) + \log|W|</script><p>Taking a close look at the above equation may find that the logarithm of likelihood function of Maximum Likelihood Estimation  is same with the above equation, so that MLE techniques can be applied to solve out $W$. However, there is still a hurdle not mentioned before, that is PDF of the source $s$ is unknown.  </p>
<p>Recall that, Given a real-valued random variable $z$ with distribution function $p_z(z)$ , it’s  cumulative distribution function(cdf)  is given by $F(z_0) = P(z\le z_0 )=\int_{-\infty}^{z_0}p_z(z)dz $. Also , the distribution function of $x$ can be found from cdf by taking its derivative: $p_z(z) = F^{‘}(z)$ .</p>
<p>Thus, instead of directly specifying the distribution function of source $s$ , a cdf for source $s$ can be given out, then taking its derivative produces the pdf of $s$. A pdf is demand to be monotonic function increasing from zero to one. Note that we cannot choose a cdf to be the  cdf of Gaussian, since ICA does not work on Gaussian data.  Normally, the sigmoid function $g(s)=1/ (1+e^{-s})$ is chosen as a cdf for ICA-based problem. Hence, $p(s) = g(s)^{‘}$</p>
<p>Now that  the distribution function of source $s$ has been given out, the logarithm of  joint distribution of dataset $\{x^1,x^2,x^3,\dots, x^n\}$ can be rewrote as </p>
<script type="math/tex; mode=display">P(x) = \sum_{i=1}^{m}\sum_{j=1}^{n}  \log g^{'}(w_j^Tx^i) + \log{|W|}</script><p>The next thing to do is to maximize $P(x)$ with respect to the parameter $W$ , that is to say </p>
<script type="math/tex; mode=display">W_{MLS} = \mathop{\arg \max}_w {\sum_{i=1}^{m}\sum_{j=1}^{n}  \log g^{'}(w_j^Tx^i) + \log{|W|} }</script><p>using the gradient ascent optimization algorithm (more precisely, stochastic gradient ascent algorithm here  )that takes derivative of the $P(X)$ in terms $W$ for a training example $x^i$ results in the following update rules of $W$ </p>
<script type="math/tex; mode=display">W := W + \alpha \Bigg( \begin{bmatrix}1 - 2g(w_1^Tx^i)\\ 1 - 2g(w_2^Tx^i)\\ \vdots \\ 1 - 2g(w_n^Tx^i) \end{bmatrix} x^{(i)^{T}} + {(W^T)}^{-1} \Bigg)</script><p>where  $\alpha$ is the learning rate. After the algorithm converges, the $W$ would be solved out, with the formula $s^i = Wx^i$ , the original sources $s$ are supposed to be recovered.  </p>
<h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>There exists the several implementations of independent components analysis , the most efficient and popular is FastICA originally invented by Aapo Hyvärinen at <a href="https://en.wikipedia.org/wiki/Helsinki_University_of_Technology" target="_blank" rel="external">Helsinki University of Technology</a>. In this section, a dummy example of ICA applied to the practical problem will be presented in which FastICA implemented by  <code>sklearn</code> is supposed to be used. </p>
<p><strong>Generate simulation data</strong></p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> signal</div><div class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> FastICA</div><div class="line"></div><div class="line"></div><div class="line">np.random.seed(<span class="number">0</span>)</div><div class="line">n_samples = <span class="number">2000</span></div><div class="line">time = np.linspace(<span class="number">0</span>, <span class="number">8</span>, n_samples)</div><div class="line">s1 = np.sin(<span class="number">2</span> * time)</div><div class="line">s2 = np.sign(np.sin(<span class="number">3</span> * time))</div><div class="line">s3 = signal.sawtooth(<span class="number">2</span> * np.pi * time)</div><div class="line"></div><div class="line">S = np.c_[s1, s2, s3]</div><div class="line">S += <span class="number">0.2</span> * np.random.normal(size=S.shape)</div><div class="line">S /= S.std(axis=<span class="number">0</span>)</div><div class="line">A = np.array([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">0.5</span>, <span class="number">2</span>, <span class="number">1.0</span>],[<span class="number">1.5</span>,<span class="number">1.0</span>,<span class="number">2.0</span>]])</div><div class="line">X = np.dot(S, A.T)</div></pre></td></tr></table></figure>
<p><strong>Recover sources</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">ica = FastICA(n_components=<span class="number">3</span>)</div><div class="line">S_ = ica.fit_transform(X)</div><div class="line">A_ = ica.mixing_</div><div class="line">plt.figure(figsize=(<span class="number">9</span>,<span class="number">6</span>))</div><div class="line">models = [X, S, S_]</div><div class="line">names = [<span class="string">'Observations(mixied signal)'</span>,</div><div class="line">         <span class="string">'True Sources'</span>,</div><div class="line">         <span class="string">'ICA estimated sources'</span>]</div><div class="line">colors = [<span class="string">'red'</span>,<span class="string">'steelblue'</span>,<span class="string">'orange'</span>]</div><div class="line"><span class="keyword">for</span> ii,(model,name) <span class="keyword">in</span> enumerate(zip(models,names),<span class="number">1</span>):</div><div class="line">  plt.subplot(<span class="number">4</span>,<span class="number">1</span>,ii)</div><div class="line">  plt.title(name)</div><div class="line">  <span class="keyword">for</span> sig, color <span class="keyword">in</span> zip(model.T, colors):</div><div class="line">    plt.plot(sig, color=color)</div><div class="line">    plt.tight_layout()</div><div class="line">    plt.show()</div></pre></td></tr></table></figure>
<p>The Fig 1 shows the recovery results. it can be seen from the results, ICA is not able to recover the order and the amplitude of the sources, in most cases it doesn’t matter such that ICA is accessible to be applied. </p>
<p><img src="/home/jell/Desktop/ica_dummy_example.png" alt=""></p>
<center>Fig. 1. the recovered sources</center>

<h3 id="ICA-Limitations"><a href="#ICA-Limitations" class="headerlink" title="ICA Limitations"></a>ICA Limitations</h3><ul>
<li><p>In the application we are not concerned with the order of the source signals, ICA can be applied to, in other words,  <code>ICA cannot recover the order of the source signals from the observation.</code> For instance, in the cocktail party problem, when recovering the source signals, the order of the speaker ‘s sound arriving the microphone  does not matter. </p>
</li>
<li><p><code>ICA cannot recover the genuine amplitude of the source signals</code>. Also take cocktail party problem for example, the amplitude of the source speakers sound signals recovered from the observation $X=\{x_1,x_2, x_3, \dots, x_n\}$ does not matter, the amplitude here just indicates the volume of the speaker’s sound, with the lower or higher volume sound we still can recognize what the speakers are talking about. Furthermore, the positive or negative signs of  the sound signals are sounded to be identical  when played on a speaker.</p>
</li>
<li><p><code>ICA cannot work with the source signals according to Gaussian distribution</code>. Since the contour of PDF of Gaussian signal  is rotationally symmetric with respect to the origin  in the coordinate, when the specific rotational operation performed on the signal, given only the observation $X$, there is no way to find out the rotational operation being performed on. More concretely, consider an example in which  the number of the source signals and the observation point are same that is 2,  and source signal $s_i \sim  N(0,I)$ , here I is $2x2$ identity matrix.</p>
<p>Now, suppose we have some observations $x = As$, where $A$ is the mixing matrix. The distribution of $x$ is supposed to be Gaussian, with zero mean and covariance $E[xx^T] = E[Ass^TA]=AA^T$. Then, suppose there exists an arbitrary orthogonal matrix $R$(normally, a rotation/reflection matrix), so that $RR^T = RR^T=I$, and let $A^{‘} = AR $. Now, the mixing matrix $A$ is temporarily replaced with $A^{‘}$, the corresponding observations would be $x^{‘}=A^{‘}s$. Speculating the distribution of $x^{‘}$  concludes that the $x^{‘}$ is also Gaussian with zero mean and covariance $E[x^{‘}(x^{‘})^T] = E[A^{‘}ss^T(A^{‘})^T]=E[ARss^T(AR)^T]=ARR^TA^T=AA^T$. Thus, whether the mixing matrix is $A$ or $A^{‘}$, we always would observe data from a $N(0, AA^T)$ distribution. As a result, there is no way to tell if the source signals were mixed using $A$ or $A^{‘}$. So an arbitrary rotational component in the mixing matrix is impossible to recover from the observation, and we cannot recover the original source signals. </p>
</li>
</ul>
</div></article></div></main><footer><div class="paginator"><a href="/2018/11/05/a-brief-introduction-to-feature-selection-methods/" class="prev">PREV</a><a href="/2018/09/11/mle-vs-lse/" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2019 <a href="https://free-free.github.io">infinite.ft</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>