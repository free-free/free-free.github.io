<!DOCTYPE html><html lang="en,zh-cn,default"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Maximum Likelihood Estimation vs Least Square Estimation · Infinite.ft的博客</title><meta name="description" content="Maximum Likelihood Estimation vs Least Square Estimation - infinite.ft"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://free-free.github.io/atom.xml" title="Infinite.ft的博客"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">HOME</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/categories/math" target="_self" class="nav-list-link">MATH</a></li><li class="nav-list-item"><a href="/categories/tech" target="_self" class="nav-list-link">TECH</a></li><li class="nav-list-item"><a href="/categories/mylife" target="_self" class="nav-list-link">NOTES</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Maximum Likelihood Estimation vs Least Square Estimation</h1><div class="post-info">Sep 11, 2018</div><div class="post-content"><p>When it comes to the parameter estimation in machine learning, the several estimation methods might quickly come into your mind, like Maximum Likelihood Estimation(MLE) , Maximum a Posterior(MAP), Bayesian Inference, and even Least Square Estimation(LSE). At the beginning of your trip to understand them, I guess there might exist some puzzles in your head regarding of  the relationships of those parameter estimation approaches, here an answer need to being confirmed that <strong>the estimation methods have mentioned above are exactly related to each other, in essence,  from the mathematical view each two of them are identical, in general ,given that some pre-conditions be satisfied</strong>. In this post, MLE and LSE are desired to be our focus, where the theory details will be introduced and the implementations in Python will be presented.</p>
<h3 id="Maximum-Likelihood-Estimation-MLE"><a href="#Maximum-Likelihood-Estimation-MLE" class="headerlink" title="Maximum Likelihood Estimation(MLE)"></a>Maximum Likelihood Estimation(MLE)</h3><p>MLE is a parameter approach and  designed to estimate the parameter values in the light of a set of data $X$ observed from an unknown distribution process, it is here that some assumptions about the dataset $X$ be  made, that  is each data point $x_i $ in $X$ must be independent from the statistical view. MLE estimates parameter by maximizing the likelihood function in which the parameter  value of dataset’s distribution be obtained, <strong>the likelihood function indicates how likely the observed data is as a function of possible parameter values.</strong> In a nutshell, MLS is responsible for finding a specific distribution that can best explain the observed data with respect to a specific type of distribution. The literal explanation can always be  loosely substantial , in the following content some mathematical formulations would help to incorporate the understanding of MLE.</p>
<p> Given a set of sample points $X = \{x_1,x_2,x_3,\dots, x_N\}$, where $N$ denotes the sample size, each point is statistically independent with respect to the other points in $X$ which means $x_i$ is a sample from an independent random variable $X_i$, suppose the probability dense function(PDF) or the probability of $x_i$ is a function of the parameter $\theta$ , that is $p(x_i | \theta)$, the likelihood function of $X$, also as the joint probability (density) of  X, is defined as</p>
<script type="math/tex; mode=display">L(\theta) = \prod_{x_i \in X}{p(x_i|\theta)}</script><p>Now that we have the likelihood function, maximizing the likelihood function $L(\theta)$ ensues, the estimation  value of $\theta$ is obtained by the solution of the following equation</p>
<script type="math/tex; mode=display">\theta_{MLE} = \mathop{\arg\max}_{\theta } L(\theta)</script><p>With the above equation, optimization algorithms can be applied the above function to solve out the value of parameter $\theta$, the gradient ascent algorithm is assumed here. sometimes, because of the complicated differentiation process of the product expression in the likelihood function, some tricks are used , taking the natural logarithm of $L(\theta)$ is supposed to alleviate it ,then the logarithm of likelihood function looks like </p>
<script type="math/tex; mode=display">\ell(\theta) = \log{L(\theta)} = \sum_{x_i \in X}{\log{p(x_i|\theta)}}</script><p>so the Maximum Likelihood Estimates of $\theta$ is as follows</p>
<script type="math/tex; mode=display">\theta_{MLE} = \mathop{\arg\max}_{\theta} {\sum_{x_i \in X}{\log{p(x_i|\theta)}}}</script><p>Up to this point, suppose you now understand what does MLS  do. From here, an example of MLS will be demonstrated.</p>
<h3 id="Least-Square-Estimate"><a href="#Least-Square-Estimate" class="headerlink" title="Least Square Estimate"></a>Least Square Estimate</h3></div></article></div></main><footer><div class="paginator"><a href="/2018/09/17/independent-components-analysis/" class="prev">PREV</a><a href="/2018/09/09/a-collection-of-heuristic-articles-about-some-complicated-and-cryptic-concepts/" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2019 <a href="https://free-free.github.io">infinite.ft</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>