<!DOCTYPE html><html lang="en,zh-cn,default"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> A Brief Introduction of Several Useful Convolutional Networks · Infinite.ft的博客</title><meta name="description" content="A Brief Introduction of Several Useful Convolutional Networks - infinite.ft"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://free-free.github.io/atom.xml" title="Infinite.ft的博客"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">HOME</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/categories/math" target="_self" class="nav-list-link">MATH</a></li><li class="nav-list-item"><a href="/categories/tech" target="_self" class="nav-list-link">TECH</a></li><li class="nav-list-item"><a href="/categories/mylife" target="_self" class="nav-list-link">NOTES</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">A Brief Introduction of Several Useful Convolutional Networks</h1><div class="post-info">Nov 8, 2017</div><div class="post-content"><p>This  article is the course handout for Stanford University CS231n’s. it mainly contains a brief introduction of several common Convolutional Networks.</p>
<ul>
<li><p><strong>LeNet</strong></p>
<p>The first successful applications of Convolutional Networks were developed by Yann LeCun in 1990’s.Of these, the best known is the LeNet architecture that was used in read zip codes, digits,etc.</p>
<p><em>paper</em> : <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf" target="_blank" rel="external"><em>donwload</em></a></p>
</li>
<li><p><strong>AlexNet</strong></p>
<p>The first work that popularized Convolutional Networks in Computer Vision was the AlexNet,developed by Alex Krizhevsky,llya Sutskever and Geoff Hinton. The AlexNet was submitted to the  <a href="http://www.image-net.org/challenges/LSVRC/2014/" target="_blank" rel="external">ImageNet ILSVRC challenge</a> in 2012 and significantly outperformed  the second runner-up(top 5 error of 16% compared to runner-up with 26% error). The Network had a very similar architecture to LeNet, but was deeper,bigger,and featured Convolutional Layers stacked on top of each other(previously it was common to only have single CONV layer always immediately followed by a POOL layer)</p>
<p><em>paper</em> : <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" target="_blank" rel="external"><em>download</em></a></p>
</li>
<li><p><strong>ZF Net</strong></p>
<p>The ILSVRC 2013 winner was a Convolutional Network from Matthew Zeiler and Rob Fergus. It became known as the ZFNet(short for Zerler &amp; Fergus Net). It was an improvement on AlexNet by tweaking the architecture hyperparameters, in particular by expanding the size of the middle convolutional layers and making the stride and filter size on the first layer smaller</p>
<p><em>paper</em> : <a href="http://arxiv.org/abs/1311.2901" target="_blank" rel="external"><em>download</em></a></p>
</li>
<li><p><strong>GoogLeNet</strong></p>
<p>The ILSVRC 2014 winner was a Convolutional Network from Szegedy et al. from Google. Its main contribution was the development of an <em>Inception Module</em> that dramatically reduced the number of parameters in the network(4M, compared to AlexNet with 60M). Additionally, this paper uses Average Pooling instead of Fully Connected layers at the top of the ConvNet, eliminating a large amount of parameters that do not seem to matter much. There are also several follow up versions to the GoogLeNet,most recently <a href="http://arxiv.org/abs/1602.07261" target="_blank" rel="external">Inception-v4</a>.</p>
<p><em>paper</em> : <a href="http://arxiv.org/abs/1409.4842" target="_blank" rel="external"><em>download</em></a></p>
</li>
<li><p><strong>VGGNet</strong></p>
<p>The runner-up in ILSVRC 2014 was the network from Karen Simonyan and Andrew Zisserman that became known as the VGGNet. Its main contribution was in showing that the depth of the network is a critical component for good performance. Their final best network contains 16 CONV/FC layers and, appealingly , features an extremely homogeneous architecture that only performs 3x3 convolutions and 2x2 pooling from the beginning to the end .Their <a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/" target="_blank" rel="external">pretrained model</a> is available for plug and play use in Caffe. A downside of the VGGNet is that it is more expensive to evaluate and uses a lot more memory and parameters(140M). Most of these parameters are  in the first fully connected layer, and it was since found that these FC layers can be removed with no performance downgrade , significantly reducing the number of necessary parameters</p>
<p><em>paper</em> : <a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/" target="_blank" rel="external"><em>donwload</em></a></p>
</li>
<li><p><strong>ResNet</strong></p>
<p>Residual Network developed by Kaiming  He et al. was the winner of ILSVRC 2015. It features special skip connections and a heavy use of batch normalization. The architecture is also missing fully connected layers at the end of the network. The reader is also referred to Kaiming’s presentations(<a href="https://www.youtube.com/watch?v=1PGLj-uKT1w" target="_blank" rel="external">video</a>, <a href="http://research.microsoft.com/en-us/um/people/kahe/ilsvrc15/ilsvrc2015_deep_residual_learning_kaiminghe.pdf" target="_blank" rel="external">slides</a>), and some <a href="https://github.com/gcr/torch-residual-networks" target="_blank" rel="external">recent experiments</a> that reproduce these networks in Torch. ResNets are currently by far state of the art Convolutional Neural Network models and are the default choice for using ConvNets in practice(as of May 10, 2016). In particular, also see more recent developments that tweak the original architecture from <a href="https://arxiv.org/abs/1603.05027" target="_blank" rel="external">Kaiming He et al. Identity Mappings in Deep Residual Networks</a> (published  Marc 2016).</p>
<p><em>paper</em> : <a href="http://arxiv.org/abs/1512.03385" target="_blank" rel="external"><em>donwload</em></a></p>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">[1]Convolutional Neural Networks (CNNs / ConvNets)</a></p>
</div></article></div></main><footer><div class="paginator"><a href="/2017/11/23/a-brief-introduction-of-pytorch/" class="prev">PREV</a><a href="/2017/11/02/my-mind-pattern01/" class="next">NEXT</a></div><div class="copyright"><p>© 2015 - 2018 <a href="https://free-free.github.io">infinite.ft</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>